{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689aca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import laspy\n",
    "\n",
    "from laspy import LasData\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.cuda import amp\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "from torchsparse import SparseTensor\n",
    "\n",
    "from panoramix3D.models import Panoramix3D\n",
    "from panoramix3D.config import ModelConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31ceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.home() / 'Panoramix3D_data'\n",
    "CONFIG_FILE = '../config/model/inference.yaml'\n",
    "SAVE_CHUNKS = True\n",
    "USE_STORED = True\n",
    "SAVE_SEGMENTED = True\n",
    "SAVE_ALL_SEGMENTS = True\n",
    "CHUNK_SIZE = 15.0\n",
    "MIN_POINTS_PER_PC = 2000\n",
    "MIN_CHUNK_SIZE = 12.5\n",
    "VOXEL_SIZE = 0.3\n",
    "\n",
    "process_folder = DATA_DIR / 'to_process'\n",
    "process_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "input_folder = process_folder / 'input'\n",
    "output_folder = process_folder / 'output'\n",
    "segmented_folder = process_folder / 'segmented'\n",
    "segmented_by_folder = process_folder / 'segmented/by_folders'\n",
    "\n",
    "input_folder.mkdir(parents=True, exist_ok=True)\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "segmented_folder.mkdir(parents=True, exist_ok=True)\n",
    "segmented_by_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a886308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkerize(file: LasData, ext: str):\n",
    "    points = file.points\n",
    "    coords = np.vstack((file.x, file.y)).transpose()\n",
    "\n",
    "    idx  = np.floor_divide(coords, CHUNK_SIZE).astype(int)\n",
    "    idx -= idx.min(axis=0)\n",
    "\n",
    "    idx = np.ravel_multi_index(idx.T, idx.max(axis=0) + 1)\n",
    "    chunks = []\n",
    "    for i, unique_idx in enumerate(tqdm(np.unique(idx))):\n",
    "        chunk_points = points[idx == unique_idx]\n",
    "        if len(chunk_points) < MIN_POINTS_PER_PC:\n",
    "            continue\n",
    "        \n",
    "        min_x, max_x = chunk_points.x.min(), chunk_points.x.max()\n",
    "        min_y, max_y = chunk_points.y.min(), chunk_points.y.max()\n",
    "        if (max_x - min_x) < MIN_CHUNK_SIZE or (max_y - min_y) < MIN_CHUNK_SIZE:\n",
    "            continue\n",
    "\n",
    "        chunk_file = laspy.LasData(file.header)\n",
    "        chunk_file.points = chunk_points\n",
    "        chunks.append(chunk_file)\n",
    "        \n",
    "        if SAVE_CHUNKS:\n",
    "            chunk_file.write(output_folder / f'plot_{i}{ext}')\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9363e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:24<00:00, 41.17it/s]\n"
     ]
    }
   ],
   "source": [
    "extensions = ('.laz', '.las')\n",
    "point_clouds = sorted(\n",
    "            [f for f in input_folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in extensions],\n",
    "            key=lambda f: f.name\n",
    "        )\n",
    "\n",
    "config = ModelConfig.from_yaml(CONFIG_FILE)\n",
    "model = Panoramix3D.from_config(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "for point_cloud in point_clouds:\n",
    "    if USE_STORED:\n",
    "        chunks = [laspy.read(f) for f in output_folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in extensions]\n",
    "    else:\n",
    "        chunks = chunkerize(laspy.read(point_cloud), point_cloud.suffix.lower())\n",
    "\n",
    "    segmented_chunks = []\n",
    "\n",
    "    instance_offset = 1\n",
    "    for i, chunk in enumerate(tqdm(chunks)):\n",
    "        coords = np.vstack((chunk.x, chunk.y, chunk.z)).transpose()\n",
    "        min_coords = coords.min(axis=0)\n",
    "        coords -= min_coords\n",
    "\n",
    "        intensity = np.array(chunk.intensity)[:, None]\n",
    "        min_intensity = np.min(intensity)\n",
    "        max_intensity = np.max(intensity)\n",
    "        i_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        voxels, indices, inverse_map = sparse_quantize(coords, VOXEL_SIZE, return_index=True, return_inverse=True)\n",
    "        i_norm = i_norm[indices]\n",
    "\n",
    "        min_x, max_x = chunk.x.min(), chunk.x.max()\n",
    "        min_y, max_y = chunk.y.min(), chunk.y.max()\n",
    "\n",
    "        if len(voxels) < 100:\n",
    "            continue\n",
    "        if (max_x - min_x) < MIN_CHUNK_SIZE or (max_y - min_y) < MIN_CHUNK_SIZE:\n",
    "            continue\n",
    "        if (voxels[:, 2].max() - voxels[:, 2].min()) < 20:\n",
    "            continue\n",
    "\n",
    "        voxels = torch.tensor(voxels, dtype=torch.int).to(device)\n",
    "        batch_index = torch.zeros((voxels.shape[0], 1), dtype=torch.int, device=voxels.device)\n",
    "        voxels = torch.cat([batch_index, voxels], dim=1)\n",
    "        feat = torch.tensor(i_norm.astype(np.float32), dtype=torch.float).to(device)\n",
    "\n",
    "        inputs = SparseTensor(coords=voxels, feats=feat)\n",
    "\n",
    "        #print(f'Voxels shape: {voxels.shape}, Features shape: {feat.shape}')\n",
    "        with amp.autocast(enabled=True):\n",
    "            semantic_output_raw, specie_output_raw, _, _, _, instance_output_raw = model(inputs)\n",
    "\n",
    "        voxels = semantic_output_raw.C.cpu().numpy()\n",
    "        semantic_output = torch.argmax(semantic_output_raw.F.cpu(), dim=1).numpy()\n",
    "\n",
    "        ng_mask = semantic_output != 0\n",
    "\n",
    "        specie_output = np.zeros_like(semantic_output)\n",
    "        specie_output[ng_mask] = torch.argmax(specie_output_raw.F.cpu(), dim=1).numpy() + 1\n",
    "\n",
    "        instance_output_full = np.zeros_like(semantic_output)\n",
    "\n",
    "        if instance_output_raw.F.shape[1] > 0:\n",
    "            instance_output = torch.argmax(instance_output_raw.F.cpu(), dim=1).numpy()\n",
    "            max_label = instance_output.max()\n",
    "            instance_output_full[ng_mask] = instance_output + instance_offset\n",
    "            instance_offset += max_label + 1\n",
    "\n",
    "        semantic_output = semantic_output[inverse_map]\n",
    "        specie_output = specie_output[inverse_map]\n",
    "        instance_output = instance_output_full[inverse_map]\n",
    "\n",
    "        out_file = laspy.LasData(header=chunk.header, points=chunk.points.copy())\n",
    "        out_file.add_extra_dims([laspy.ExtraBytesParams(name=\"semantic_pred\", type=np.int16), laspy.ExtraBytesParams(name=\"species_pred\", type=np.int32), laspy.ExtraBytesParams(name=\"instance_pred\", type=np.int32)])\n",
    "        out_file.semantic_pred = semantic_output\n",
    "        out_file.species_pred = specie_output\n",
    "        out_file.instance_pred = instance_output\n",
    "\n",
    "        segmented_chunks.append(out_file)\n",
    "        if SAVE_ALL_SEGMENTS:\n",
    "            out_file.write(segmented_by_folder / f'plot_{i}.las')\n",
    "\n",
    "    with laspy.open(segmented_folder / 'combined.las', mode='w', header=segmented_chunks[0].header) as file:\n",
    "        for chunk in segmented_chunks:\n",
    "            file.write_points(chunk.points)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
