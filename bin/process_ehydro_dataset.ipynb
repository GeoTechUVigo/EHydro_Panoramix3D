{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd636a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import laspy\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77ad508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAR_DIRS = True\n",
    "COLLAPSE_CLASSES = True\n",
    "SPLIT_POINTCLOUDS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11bc69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehydro_semantics_orig = {\n",
    "    'ground': 0,\n",
    "    'track': 1,\n",
    "    'road': 2,\n",
    "    'water': 3,\n",
    "    'shrubs': 4,\n",
    "    'trees': 5,\n",
    "    'buildings': 6,\n",
    "    'misc': 7,\n",
    "    'power_lines': 8\n",
    "}\n",
    "\n",
    "ehydro_semantics_comp = {\n",
    "    'ground': 0,\n",
    "    'track': 1,\n",
    "    'water': 2,\n",
    "    'shrubs': 3,\n",
    "    'trees': 4,\n",
    "    'misc': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c99a52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehydro_dataset = Path.home() / 'Panoramix3D_data' / 'datasets' / 'EHydroDataset'\n",
    "\n",
    "hybrid_plots_dir = ehydro_dataset / 'raw' / 'hybrid_plots'\n",
    "real_plots_dir = ehydro_dataset / 'raw' / 'real_plots'\n",
    "\n",
    "train_raw_dir = hybrid_plots_dir / 'output'\n",
    "val_raw_dir = real_plots_dir / 'val'\n",
    "test_raw_dir = real_plots_dir / 'test'\n",
    "\n",
    "processed_dir = ehydro_dataset / 'processed'\n",
    "train_dir = processed_dir / 'train'\n",
    "val_dir = processed_dir / 'val'\n",
    "test_dir = processed_dir / 'test'\n",
    "\n",
    "if CLEAR_DIRS:\n",
    "    if processed_dir.exists():\n",
    "        shutil.rmtree(processed_dir)\n",
    "\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkerize_four(las_data):\n",
    "    xy = np.stack([las_data.x, las_data.y], axis=1)\n",
    "    center = xy.mean(axis=0)\n",
    "\n",
    "    return [\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] < center[1]),\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] < center[1])\n",
    "    ]\n",
    "\n",
    "def collapse_classes(las_data):\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['ground']] = ehydro_semantics_comp['ground']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['track']] = ehydro_semantics_comp['track']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['road']] = ehydro_semantics_comp['track']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['water']] = ehydro_semantics_comp['water']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['shrubs']] = ehydro_semantics_comp['shrubs']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['trees']] = ehydro_semantics_comp['trees']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['buildings']] = ehydro_semantics_comp['misc']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['misc']] = ehydro_semantics_comp['misc']\n",
    "    las_data.semantic_gt[las_data.semantic_gt == ehydro_semantics_orig['power_lines']] = ehydro_semantics_comp['misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac881640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split(raw_dir: Path, dest_dir: Path):\n",
    "    files = list(raw_dir.glob('*.las'))\n",
    "    for file in tqdm(files):\n",
    "        with laspy.open(file) as las:\n",
    "            las_data = las.read()\n",
    "            if COLLAPSE_CLASSES:\n",
    "                collapse_classes(las_data)\n",
    "            if SPLIT_POINTCLOUDS:\n",
    "                chunks = chunkerize_four(las_data)\n",
    "                for i, mask in enumerate(chunks):\n",
    "                    chunk = las_data.points[mask]\n",
    "                    out = laspy.create(point_format=las_data.point_format, file_version=las_data.header.version)\n",
    "                    out.header.scales = las_data.header.scales\n",
    "                    out.header.offsets = np.array([0.0, 0.0, 0.0])\n",
    "                    out.points = chunk\n",
    "\n",
    "                    x = chunk.x\n",
    "                    y = chunk.y\n",
    "                    z = chunk.z\n",
    "\n",
    "                    out.x = x - np.array(x).min()\n",
    "                    out.y = y - np.array(y).min()\n",
    "                    out.z = z - np.array(z).min()\n",
    "\n",
    "                    _, out.instance_gt = np.unique(chunk.instance_gt, return_inverse=True)\n",
    "                    out.write(dest_dir / f\"{file.stem}_chunk_{i}.las\")\n",
    "            else:\n",
    "                las_data.write(dest_dir / file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "defccddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660/660 [00:16<00:00, 40.16it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 82.50it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "process_split(train_raw_dir, train_dir)\n",
    "process_split(val_raw_dir, val_dir)\n",
    "process_split(test_raw_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d604a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verificando train (2640 archivos) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2640/2640 [00:00<00:00, 2717.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verificando val (128 archivos) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 2826.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verificando test (0 archivos) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Todas las nubes pasaron la validación\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ehydro_semantics = ehydro_semantics_comp if COLLAPSE_CLASSES else ehydro_semantics_orig\n",
    "\n",
    "errors = []\n",
    "\n",
    "for split_name, split_dir in [('train', train_dir), ('val', val_dir), ('test', test_dir)]:\n",
    "    files = list(split_dir.glob('*.las'))\n",
    "    print(f\"\\n=== Verificando {split_name} ({len(files)} archivos) ===\")\n",
    "    \n",
    "    for las_file in tqdm(files):\n",
    "        las = laspy.read(las_file)\n",
    "        \n",
    "        # Verificar puntos NO-tree\n",
    "        non_tree_mask = las.semantic_gt != ehydro_semantics['trees']\n",
    "        non_tree_instance_invalid = (las.instance_gt[non_tree_mask] != 0).any()\n",
    "        non_tree_species_invalid = (las.species_gt[non_tree_mask] != 0).any()\n",
    "        \n",
    "        if non_tree_instance_invalid:\n",
    "            errors.append(f\"{split_name}/{las_file.name}: Puntos NO-tree con instance_gt != 0\")\n",
    "        if non_tree_species_invalid:\n",
    "            errors.append(f\"{split_name}/{las_file.name}: Puntos NO-tree con species_gt != 0\")\n",
    "        \n",
    "        # Verificar puntos tree\n",
    "        tree_mask = las.semantic_gt == ehydro_semantics['trees']\n",
    "        if tree_mask.any():\n",
    "            tree_instance_invalid = (las.instance_gt[tree_mask] <= 0).any()\n",
    "            tree_species_invalid = (las.species_gt[tree_mask] <= 0).any()\n",
    "            \n",
    "            if tree_instance_invalid:\n",
    "                errors.append(f\"{split_name}/{las_file.name}: Puntos tree con instance_gt == 0\")\n",
    "            if tree_species_invalid:\n",
    "                errors.append(f\"{split_name}/{las_file.name}: Puntos tree con species_gt == 0\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n❌ ERRORES ENCONTRADOS ({len(errors)}):\")\n",
    "    for err in errors:\n",
    "        print(f\"  - {err}\")\n",
    "else:\n",
    "    print(\"\\n✅ Todas las nubes pasaron la validación\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
