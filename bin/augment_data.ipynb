{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f44db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from laspy import LasData\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7955e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTED_PER_POINT_CLOUD = 15\n",
    "MIN_TREES_PER_SCENE = 4\n",
    "MAX_TREES_PER_SCENE = 15\n",
    "MIN_DISTANCE = 5.0\n",
    "VAL_CHUNKS = 32\n",
    "\n",
    "YAW_RANGE = (0.0, 360.0)\n",
    "TILT_RANGE = (-2.0, 2.0)\n",
    "SCALE_RANGE = (0.9, 1.1)\n",
    "\n",
    "mixed_dataset = Path.home() / 'Panoramix3D_data' / 'datasets' / 'EHydroDataset'\n",
    "\n",
    "raw_folder = mixed_dataset / 'raw'\n",
    "grounds_folder = raw_folder / 'grounds'\n",
    "trees_folder = raw_folder / 'trees'\n",
    "\n",
    "processed_folder = mixed_dataset / 'processed'\n",
    "train_folder = processed_folder / 'train'\n",
    "val_folder = processed_folder / 'val'\n",
    "test_folder = processed_folder / 'test'\n",
    "\n",
    "grounds_folder.mkdir(parents=True, exist_ok=True)\n",
    "trees_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "train_folder.mkdir(parents=True, exist_ok=True)\n",
    "val_folder.mkdir(parents=True, exist_ok=True)\n",
    "test_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = list((raw_folder / 'train').glob('*.las'))\n",
    "\n",
    "ground_counter = 0\n",
    "tree_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a3d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_las(file, points, folder):\n",
    "    out = laspy.create(point_format=file.point_format, file_version=file.header.version)\n",
    "    out.header.scales = file.header.scales\n",
    "    out.header.offsets = np.array([0.0, 0.0, 0.0])\n",
    "    out.points = points\n",
    "\n",
    "    x = points.x\n",
    "    y = points.y\n",
    "    z = points.z\n",
    "\n",
    "    out.x = x - np.array(x).mean()\n",
    "    out.y = y - np.array(y).mean()\n",
    "    out.z = z - np.array(z).min()\n",
    "\n",
    "    global ground_counter, tree_counter\n",
    "    if folder == 'ground':\n",
    "        out.write(grounds_folder / f'ground_{ground_counter}.las')\n",
    "        ground_counter += 1\n",
    "    else:\n",
    "        out.write(trees_folder / f'tree_{tree_counter}.las')\n",
    "        tree_counter += 1\n",
    "\n",
    "def find_n_points(points, n):\n",
    "    if len(points) <= n:\n",
    "        return points\n",
    "    \n",
    "    choosen_idx = []\n",
    "    idx = np.arange(len(points))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = list(idx)\n",
    "\n",
    "    while len(choosen_idx) < n and len(idx) > 0:\n",
    "        candidate = idx.pop()\n",
    "        too_close = False\n",
    "        for c in choosen_idx:\n",
    "            dist = np.linalg.norm(points[c] - points[candidate])\n",
    "            if dist < MIN_DISTANCE:\n",
    "                too_close = True\n",
    "                break\n",
    "\n",
    "        if not too_close:\n",
    "            choosen_idx.append(candidate)\n",
    "\n",
    "    return points[np.array(choosen_idx)]\n",
    "\n",
    "def load_trees(paths: Path) -> List[LasData]:\n",
    "    trees = []\n",
    "    for path in paths:\n",
    "        with laspy.open(path) as f:\n",
    "            las_data = f.read()\n",
    "\n",
    "        trees.append(las_data)\n",
    "    return trees\n",
    "\n",
    "def chunkerize_four(file):\n",
    "    xy = np.stack([file.x, file.y], axis=1)\n",
    "    center = xy.mean(axis=0)\n",
    "\n",
    "    return [\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] < center[1]),\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] < center[1])\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b908b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:30<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    with laspy.open(file) as f:\n",
    "        ground = f.read()\n",
    "\n",
    "    ground_mask = ground.semantic_gt == 0\n",
    "    ground_points = ground.points[ground_mask]\n",
    "    save_las(ground, ground_points, 'ground')\n",
    "\n",
    "    unique_ids = np.unique(ground.instance_gt)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "\n",
    "    for id in unique_ids:\n",
    "        tree_mask = ground.instance_gt == id\n",
    "        tree_xyz = ground.points[tree_mask]\n",
    "        save_las(ground, tree_xyz, 'tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85fa62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:17<00:00,  3.69it/s]\n"
     ]
    }
   ],
   "source": [
    "ground_files = list((grounds_folder).glob('*.las'))\n",
    "tree_files = list((trees_folder).glob('*.las'))\n",
    "\n",
    "for file in tqdm(range(len(ground_files))):\n",
    "    # print(ground_files[file])\n",
    "    with laspy.open(ground_files[file]) as f:\n",
    "        ground = f.read()\n",
    "\n",
    "    ground_scale = np.random.uniform(*SCALE_RANGE)\n",
    "    ground_yaw = np.radians(np.random.uniform(*YAW_RANGE))\n",
    "    ground_tilt = np.radians(np.random.uniform(*TILT_RANGE))\n",
    "\n",
    "    xyz = np.array(ground.xyz) * ground_scale\n",
    "    xyz = xyz @ np.array([\n",
    "        [np.cos(ground_yaw), -np.sin(ground_yaw), 0],\n",
    "        [np.sin(ground_yaw), np.cos(ground_yaw), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    xyz = xyz @ np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(ground_tilt), -np.sin(ground_tilt)],\n",
    "        [0, np.sin(ground_tilt), np.cos(ground_tilt)]\n",
    "    ])\n",
    "\n",
    "    ground.xyz = xyz\n",
    "\n",
    "    for copy in range(AUGMENTED_PER_POINT_CLOUD):\n",
    "        num_trees = np.random.randint(MIN_TREES_PER_SCENE, MAX_TREES_PER_SCENE + 1)\n",
    "        choosen_points = find_n_points(xyz[ground.semantic_gt == 0], num_trees)\n",
    "        choosen_trees = np.random.choice(tree_files, num_trees, replace=False)\n",
    "        trees = load_trees(choosen_trees)\n",
    "\n",
    "        out_path = raw_folder / 'aug' / f'{ground_files[file].stem}_aug_{copy}.las'\n",
    "        with laspy.open(out_path, mode='w', header=ground.header) as w:\n",
    "            w.write_points(ground.points)\n",
    "\n",
    "            for id, (point, tree) in enumerate(zip(choosen_points, trees)):\n",
    "                tree.instance_gt = np.full_like(tree.instance_gt, fill_value=id + 1)\n",
    "                tree.semantic_gt = np.full_like(tree.semantic_gt, fill_value=5)\n",
    "\n",
    "                tree_scale = np.random.uniform(*SCALE_RANGE)\n",
    "                tree_yaw = np.radians(np.random.uniform(*YAW_RANGE))\n",
    "                tree_tilt = np.radians(np.random.uniform(*TILT_RANGE))\n",
    "                \n",
    "                tree_xyz = np.array(tree.xyz) * tree_scale\n",
    "                tree_xyz = tree_xyz @ np.array([\n",
    "                    [np.cos(tree_yaw), -np.sin(tree_yaw), 0],\n",
    "                    [np.sin(tree_yaw), np.cos(tree_yaw), 0],\n",
    "                    [0, 0, 1]\n",
    "                ])\n",
    "                tree_xyz = tree_xyz @ np.array([\n",
    "                    [1, 0, 0],\n",
    "                    [0, np.cos(tree_tilt), -np.sin(tree_tilt)],\n",
    "                    [0, np.sin(tree_tilt), np.cos(tree_tilt)]\n",
    "                ])\n",
    "\n",
    "                tree_xyz += point\n",
    "                tree.xyz = tree_xyz\n",
    "\n",
    "                w.write_points(tree.points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a905f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.90s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.90s/it]\n"
     ]
    }
   ],
   "source": [
    "folders = ['train', 'val', 'test', 'aug']\n",
    "for folder in tqdm(folders):\n",
    "    split_folder = raw_folder / folder\n",
    "    files = list((split_folder).glob('*.las'))\n",
    "\n",
    "    for file in files:\n",
    "        with laspy.open(file) as f:\n",
    "            las = f.read()\n",
    "\n",
    "        masks = chunkerize_four(las)\n",
    "        for i, mask in enumerate(masks):\n",
    "            chunk = las.points[mask]\n",
    "            out = laspy.create(point_format=las.point_format, file_version=las.header.version)\n",
    "            out.header.scales = las.header.scales\n",
    "            out.header.offsets = np.array([0.0, 0.0, 0.0])\n",
    "            out.points = chunk\n",
    "\n",
    "            x = chunk.x\n",
    "            y = chunk.y\n",
    "            z = chunk.z\n",
    "\n",
    "            out.x = x - np.array(x).min()\n",
    "            out.y = y - np.array(y).min()\n",
    "            out.z = z - np.array(z).min()\n",
    "\n",
    "            _, out.instance_gt = np.unique(chunk.instance_gt, return_inverse=True)\n",
    "\n",
    "            out_folder = (processed_folder / folder) if folder != 'aug' else (processed_folder / 'train')\n",
    "            out.write(out_folder / f'{file.stem}_chunk_{i}.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a63a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Archivos encontrados en train: 3840\n",
      "ðŸ” Analizando clases semÃ¡nticas en cada archivo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando clases:   0%|          | 0/3840 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando clases: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3840/3840 [00:01<00:00, 2063.99it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Seleccionando archivos para cubrir todas las clases...\n",
      "  âœ… Grounds_seg_10_2025-11-17_19h51_34_773_aug_3_chunk_2.las aporta clases: [0, 2, 4, 5, 6, 7]\n",
      "  âœ… Grounds_seg_5_2025-11-17_19h52_28_683_aug_12_chunk_0.las aporta clases: [8]\n",
      "  âœ… Grounds_seg_3_2025-11-17_19h53_02_211_aug_6_chunk_1.las aporta clases: [1, 3]\n",
      "\n",
      "ðŸŽ¯ Archivos seleccionados para mover a val: 32\n",
      "ðŸ“Š Clases cubiertas en val: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "âœ… Todas las clases [0-8] estÃ¡n representadas en val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moviendo archivos a val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 7250.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Se movieron 32 archivos de train a val\n",
      "ðŸ“Š Archivos restantes en train: 3808\n",
      "ðŸ“Š Archivos totales en val: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_files = list((processed_folder / 'train').glob('*.las'))\n",
    "print(f\"ðŸ“ Archivos encontrados en train: {len(train_files)}\")\n",
    "\n",
    "# Analizar quÃ© clases contiene cada archivo\n",
    "print(\"ðŸ” Analizando clases semÃ¡nticas en cada archivo...\")\n",
    "file_classes = {}\n",
    "all_classes = set(range(9))  # Clases [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "for train_file in tqdm(train_files, desc=\"Analizando clases\"):\n",
    "    with laspy.open(train_file) as f:\n",
    "        las = f.read()\n",
    "    unique_classes = set(np.unique(las.semantic_gt))\n",
    "    file_classes[train_file] = unique_classes\n",
    "\n",
    "# SelecciÃ³n estratÃ©gica para asegurar que val tenga todas las clases\n",
    "selected_files = []\n",
    "val_classes_covered = set()\n",
    "\n",
    "# Primero, seleccionar archivos que aporten clases faltantes\n",
    "print(\"ðŸ“Š Seleccionando archivos para cubrir todas las clases...\")\n",
    "for train_file, classes in sorted(file_classes.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    if len(selected_files) >= VAL_CHUNKS:\n",
    "        break\n",
    "    \n",
    "    # Si este archivo aporta clases nuevas o aÃºn no tenemos todas las clases\n",
    "    new_classes = classes - val_classes_covered\n",
    "    if new_classes or val_classes_covered != all_classes:\n",
    "        selected_files.append(train_file)\n",
    "        val_classes_covered.update(classes)\n",
    "        if new_classes:\n",
    "            print(f\"  âœ… {train_file.name} aporta clases: {sorted(new_classes)}\")\n",
    "\n",
    "# Si aÃºn no tenemos suficientes archivos, aÃ±adir aleatoriamente\n",
    "remaining_files = [f for f in train_files if f not in selected_files]\n",
    "if len(selected_files) < VAL_CHUNKS and remaining_files:\n",
    "    additional_needed = VAL_CHUNKS - len(selected_files)\n",
    "    additional_files = random.sample(remaining_files, min(additional_needed, len(remaining_files)))\n",
    "    selected_files.extend(additional_files)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Archivos seleccionados para mover a val: {len(selected_files)}\")\n",
    "print(f\"ðŸ“Š Clases cubiertas en val: {sorted(val_classes_covered)}\")\n",
    "missing_classes = all_classes - val_classes_covered\n",
    "if missing_classes:\n",
    "    print(f\"âš ï¸  Clases NO cubiertas en val: {sorted(missing_classes)}\")\n",
    "else:\n",
    "    print(f\"âœ… Todas las clases [0-8] estÃ¡n representadas en val\")\n",
    "\n",
    "# Mover los archivos seleccionados\n",
    "moved_count = 0\n",
    "for file_path in tqdm(selected_files, desc=\"Moviendo archivos a val\"):\n",
    "    try:\n",
    "        destination = processed_folder / 'val' / file_path.name\n",
    "        shutil.move(str(file_path), str(destination))\n",
    "        moved_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error moviendo {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Se movieron {moved_count} archivos de train a val\")\n",
    "print(f\"ðŸ“Š Archivos restantes en train: {len(list((processed_folder / 'train').glob('*.las')))}\")\n",
    "print(f\"ðŸ“Š Archivos totales en val: {len(list((processed_folder / 'val').glob('*.las')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc183f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando 960 archivos en la carpeta 'aug'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking aug files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:01<00:00, 855.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESUMEN GLOBAL:\n",
      "IDs Ãºnicos encontrados en todo el conjunto 'aug': [0, 1, 2]\n",
      "Rango de IDs: 0 - 2\n",
      "Total de IDs Ãºnicos diferentes: 3\n",
      "\n",
      "ðŸ“‹ DETALLES POR ARCHIVO (primeros 10):\n",
      "Grounds_seg_0_2025-11-17_19h52_28_526_aug_7.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_5_2025-11-17_19h53_02_269_aug_3.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_10_2025-11-17_19h52_28_825_aug_13.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_6_2025-11-17_19h53_19_952_aug_12.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_3_2025-11-17_19h53_26_029_aug_0.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_0_2025-11-17_19h53_19_793_aug_2.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_6_2025-11-17_19h53_26_102_aug_7.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_9_2025-11-17_19h52_28_801_aug_11.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_3_2025-11-17_19h52_07_046_aug_14.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Grounds_seg_1_2025-11-17_19h51_54_601_aug_5.las: IDs [0 1 2] (rango: 0-2, total: 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar IDs Ãºnicos en todas las nubes de aug\n",
    "aug_folder = raw_folder / 'aug'\n",
    "aug_files = list(aug_folder.glob('*.las'))\n",
    "\n",
    "print(f\"Analizando {len(aug_files)} archivos en la carpeta 'aug'...\")\n",
    "\n",
    "all_unique_ids = set()\n",
    "file_id_info = []\n",
    "\n",
    "for aug_file in tqdm(aug_files, desc=\"Checking aug files\"):\n",
    "    with laspy.open(aug_file) as f:\n",
    "        las = f.read()\n",
    "    \n",
    "    unique_ids = np.unique(las.semantic_gt)\n",
    "    all_unique_ids.update(unique_ids)\n",
    "    \n",
    "    file_id_info.append({\n",
    "        'file': aug_file.name,\n",
    "        'unique_ids': unique_ids,\n",
    "        'min_id': np.min(unique_ids),\n",
    "        'max_id': np.max(unique_ids),\n",
    "        'num_unique': len(unique_ids)\n",
    "    })\n",
    "\n",
    "print(f\"\\nðŸ“Š RESUMEN GLOBAL:\")\n",
    "print(f\"IDs Ãºnicos encontrados en todo el conjunto 'aug': {sorted(all_unique_ids)}\")\n",
    "print(f\"Rango de IDs: {min(all_unique_ids)} - {max(all_unique_ids)}\")\n",
    "print(f\"Total de IDs Ãºnicos diferentes: {len(all_unique_ids)}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ DETALLES POR ARCHIVO (primeros 10):\")\n",
    "for i, info in enumerate(file_id_info[:10]):\n",
    "    print(f\"{info['file']}: IDs {info['unique_ids']} (rango: {info['min_id']}-{info['max_id']}, total: {info['num_unique']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 3808 archivos en train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verificando consistencia semantic/instance:   0%|          | 0/3808 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verificando consistencia semantic/instance: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3808/3808 [00:01<00:00, 2385.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“Š RESUMEN:\n",
      "Total de archivos verificados: 3808\n",
      "Archivos con problemas: 0\n",
      "Archivos correctos: 3808\n",
      "\n",
      "âœ… Todos los archivos cumplen las reglas:\n",
      "   - semantic_gt == 5 â†’ instance_gt != 0\n",
      "   - semantic_gt != 5 â†’ instance_gt == 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar consistencia semantic_gt vs instance_gt en train\n",
    "train_files = list((processed_folder / 'train').glob('*.las'))\n",
    "print(f\"Verificando {len(train_files)} archivos en train...\")\n",
    "\n",
    "problematic_files = []\n",
    "\n",
    "for train_file in tqdm(train_files, desc=\"Verificando consistencia semantic/instance\"):\n",
    "    with laspy.open(train_file) as f:\n",
    "        las = f.read()\n",
    "    \n",
    "    semantic_gt = np.array(las.semantic_gt)\n",
    "    instance_gt = np.array(las.instance_gt)\n",
    "    \n",
    "    # Regla 1: semantic_gt == 5 debe tener instance_gt != 0\n",
    "    mask_semantic_5 = semantic_gt == 5\n",
    "    mask_instance_0_but_semantic_5 = mask_semantic_5 & (instance_gt <= 0)\n",
    "    num_violations_rule1 = np.sum(mask_instance_0_but_semantic_5)\n",
    "    \n",
    "    # Regla 2: semantic_gt != 5 debe tener instance_gt == 0\n",
    "    mask_semantic_not_5 = semantic_gt != 5\n",
    "    mask_instance_not_0_but_semantic_not_5 = mask_semantic_not_5 & (instance_gt != 0)\n",
    "    num_violations_rule2 = np.sum(mask_instance_not_0_but_semantic_not_5)\n",
    "    \n",
    "    if num_violations_rule1 > 0 or num_violations_rule2 > 0:\n",
    "        problematic_files.append({\n",
    "            'file': train_file.name,\n",
    "            'rule1_violations': num_violations_rule1,  # semantic=5 pero instance=0\n",
    "            'rule2_violations': num_violations_rule2,  # semantic!=5 pero instance!=0\n",
    "            'total_points': len(semantic_gt)\n",
    "        })\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“Š RESUMEN:\")\n",
    "print(f\"Total de archivos verificados: {len(train_files)}\")\n",
    "print(f\"Archivos con problemas: {len(problematic_files)}\")\n",
    "print(f\"Archivos correctos: {len(train_files) - len(problematic_files)}\")\n",
    "\n",
    "if problematic_files:\n",
    "    print(f\"\\nâŒ ARCHIVOS PROBLEMÃTICOS (primeros 20):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i, info in enumerate(problematic_files[:20]):\n",
    "        print(f\"\\n{info['file']}:\")\n",
    "        if info['rule1_violations'] > 0:\n",
    "            pct1 = (info['rule1_violations'] / info['total_points']) * 100\n",
    "            print(f\"  âŒ Regla 1: {info['rule1_violations']} puntos con semantic=5 pero instance=0 ({pct1:.2f}%)\")\n",
    "        if info['rule2_violations'] > 0:\n",
    "            pct2 = (info['rule2_violations'] / info['total_points']) * 100\n",
    "            print(f\"  âŒ Regla 2: {info['rule2_violations']} puntos con semantic!=5 pero instance!=0 ({pct2:.2f}%)\")\n",
    "    \n",
    "    if len(problematic_files) > 20:\n",
    "        print(f\"\\n... y {len(problematic_files) - 20} archivos mÃ¡s con problemas\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Todos los archivos cumplen las reglas:\")\n",
    "    print(f\"   - semantic_gt == 5 â†’ instance_gt != 0\")\n",
    "    print(f\"   - semantic_gt != 5 â†’ instance_gt <= 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c7394dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando 3808 archivos en train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verificando consistencia semantic/species: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3808/3808 [00:01<00:00, 2755.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ“Š RESUMEN:\n",
      "Total de archivos verificados: 3808\n",
      "Archivos con problemas: 0\n",
      "Archivos correctos: 3808\n",
      "\n",
      "âœ… Todos los archivos cumplen las reglas:\n",
      "   - semantic_gt == 5 â†’ species_gt > 0\n",
      "   - semantic_gt != 5 â†’ species_gt == 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar consistencia semantic_gt vs species_gt en train\n",
    "train_files = list((processed_folder / 'train').glob('*.las'))\n",
    "print(f\"Verificando {len(train_files)} archivos en train...\")\n",
    "\n",
    "problematic_files_species = []\n",
    "\n",
    "for train_file in tqdm(train_files, desc=\"Verificando consistencia semantic/species\"):\n",
    "    with laspy.open(train_file) as f:\n",
    "        las = f.read()\n",
    "    \n",
    "    semantic_gt = np.array(las.semantic_gt)\n",
    "    species_gt = np.array(las.species_gt)\n",
    "    \n",
    "    # Regla 1: semantic_gt == 5 debe tener species_gt > 0\n",
    "    mask_semantic_5 = semantic_gt == 5\n",
    "    mask_species_0_but_semantic_5 = mask_semantic_5 & (species_gt == 0)\n",
    "    num_violations_rule1 = np.sum(mask_species_0_but_semantic_5)\n",
    "    \n",
    "    # Regla 2: semantic_gt != 5 debe tener species_gt == 0\n",
    "    mask_semantic_not_5 = semantic_gt != 5\n",
    "    mask_species_not_0_but_semantic_not_5 = mask_semantic_not_5 & (species_gt != 0)\n",
    "    num_violations_rule2 = np.sum(mask_species_not_0_but_semantic_not_5)\n",
    "    \n",
    "    if num_violations_rule1 > 0 or num_violations_rule2 > 0:\n",
    "        problematic_files_species.append({\n",
    "            'file': train_file.name,\n",
    "            'rule1_violations': num_violations_rule1,  # semantic=5 pero species=0\n",
    "            'rule2_violations': num_violations_rule2,  # semantic!=5 pero species!=0\n",
    "            'total_points': len(semantic_gt)\n",
    "        })\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“Š RESUMEN:\")\n",
    "print(f\"Total de archivos verificados: {len(train_files)}\")\n",
    "print(f\"Archivos con problemas: {len(problematic_files_species)}\")\n",
    "print(f\"Archivos correctos: {len(train_files) - len(problematic_files_species)}\")\n",
    "\n",
    "if problematic_files_species:\n",
    "    print(f\"\\nâŒ ARCHIVOS PROBLEMÃTICOS (primeros 20):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for i, info in enumerate(problematic_files_species[:20]):\n",
    "        print(f\"\\n{info['file']}:\")\n",
    "        if info['rule1_violations'] > 0:\n",
    "            pct1 = (info['rule1_violations'] / info['total_points']) * 100\n",
    "            print(f\"  âŒ Regla 1: {info['rule1_violations']} puntos con semantic=5 pero species=0 ({pct1:.2f}%)\")\n",
    "        if info['rule2_violations'] > 0:\n",
    "            pct2 = (info['rule2_violations'] / info['total_points']) * 100\n",
    "            print(f\"  âŒ Regla 2: {info['rule2_violations']} puntos con semantic!=5 pero species!=0 ({pct2:.2f}%)\")\n",
    "    \n",
    "    if len(problematic_files_species) > 20:\n",
    "        print(f\"\\n... y {len(problematic_files_species) - 20} archivos mÃ¡s con problemas\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Todos los archivos cumplen las reglas:\")\n",
    "    print(f\"   - semantic_gt == 5 â†’ species_gt > 0\")\n",
    "    print(f\"   - semantic_gt != 5 â†’ species_gt == 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular desbalance de clases en train\n",
    "train_files = list((processed_folder / 'train').glob('*.las'))\n",
    "print(f\"ðŸ“Š Calculando desbalance de clases en {len(train_files)} archivos de train...\")\n",
    "\n",
    "# Contador de puntos por clase\n",
    "class_counts = {i: 0 for i in range(9)}  # Clases [0-8]\n",
    "total_points = 0\n",
    "\n",
    "for train_file in tqdm(train_files, desc=\"Contando puntos por clase\"):\n",
    "    with laspy.open(train_file) as f:\n",
    "        las = f.read()\n",
    "    \n",
    "    semantic_gt = np.array(las.semantic_gt)\n",
    "    unique, counts = np.unique(semantic_gt, return_counts=True)\n",
    "    \n",
    "    for cls, count in zip(unique, counts):\n",
    "        if cls in class_counts:\n",
    "            class_counts[cls] += count\n",
    "    \n",
    "    total_points += len(semantic_gt)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“ˆ DISTRIBUCIÃ“N DE CLASES EN TRAIN:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total de puntos: {total_points:,}\")\n",
    "print()\n",
    "\n",
    "# Ordenar por nÃºmero de clase\n",
    "for cls in sorted(class_counts.keys()):\n",
    "    count = class_counts[cls]\n",
    "    percentage = (count / total_points) * 100 if total_points > 0 else 0\n",
    "    bar = 'â–ˆ' * int(percentage / 2)  # Barra visual (cada â–ˆ = 2%)\n",
    "    print(f\"Clase {cls}: {count:>12,} puntos ({percentage:>6.2f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ðŸ“Š ANÃLISIS DE DESBALANCE:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Clase mÃ¡s frecuente y menos frecuente\n",
    "max_class = max(class_counts, key=class_counts.get)\n",
    "min_class = min(class_counts, key=class_counts.get)\n",
    "max_count = class_counts[max_class]\n",
    "min_count = class_counts[min_class]\n",
    "\n",
    "print(f\"Clase mÃ¡s frecuente: {max_class} ({max_count:,} puntos)\")\n",
    "print(f\"Clase menos frecuente: {min_class} ({min_count:,} puntos)\")\n",
    "\n",
    "if min_count > 0:\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    print(f\"Ratio de desbalance: {imbalance_ratio:.2f}:1\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Clase {min_class} no tiene puntos!\")\n",
    "\n",
    "# Calcular pesos para balanceo (inversamente proporcional a la frecuencia)\n",
    "print(f\"\\nðŸ’¡ PESOS SUGERIDOS PARA BALANCEO (inversamente proporcional):\")\n",
    "weights = {}\n",
    "for cls in sorted(class_counts.keys()):\n",
    "    if class_counts[cls] > 0:\n",
    "        weight = total_points / (len(class_counts) * class_counts[cls])\n",
    "        weights[cls] = weight\n",
    "        print(f\"Clase {cls}: {weight:.4f}\")\n",
    "    else:\n",
    "        print(f\"Clase {cls}: N/A (sin puntos)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
