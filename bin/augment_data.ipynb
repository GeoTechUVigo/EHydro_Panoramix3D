{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f44db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from laspy import LasData\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7955e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUGMENTED_PER_POINT_CLOUD = 15\n",
    "MIN_TREES_PER_SCENE = 4\n",
    "MAX_TREES_PER_SCENE = 15\n",
    "MIN_DISTANCE = 5.0\n",
    "VAL_CHUNKS = 32\n",
    "\n",
    "YAW_RANGE = (0.0, 360.0)\n",
    "TILT_RANGE = (-2.0, 2.0)\n",
    "SCALE_RANGE = (0.9, 1.1)\n",
    "\n",
    "mixed_dataset = Path.home() / 'Panoramix3D_data' / 'datasets' / 'EHydroDataset'\n",
    "\n",
    "raw_folder = mixed_dataset / 'raw'\n",
    "grounds_folder = raw_folder / 'grounds'\n",
    "trees_folder = raw_folder / 'trees'\n",
    "\n",
    "processed_folder = mixed_dataset / 'processed'\n",
    "train_folder = processed_folder / 'train'\n",
    "val_folder = processed_folder / 'val'\n",
    "test_folder = processed_folder / 'test'\n",
    "\n",
    "grounds_folder.mkdir(parents=True, exist_ok=True)\n",
    "trees_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "train_folder.mkdir(parents=True, exist_ok=True)\n",
    "val_folder.mkdir(parents=True, exist_ok=True)\n",
    "test_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = list((raw_folder / 'train').glob('*.las'))\n",
    "\n",
    "ground_counter = 0\n",
    "tree_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a3d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_las(file, points, folder):\n",
    "    out = laspy.create(point_format=file.point_format, file_version=file.header.version)\n",
    "    out.header.scales = file.header.scales\n",
    "    out.header.offsets = np.array([0.0, 0.0, 0.0])\n",
    "    out.points = points\n",
    "\n",
    "    x = points.x\n",
    "    y = points.y\n",
    "    z = points.z\n",
    "\n",
    "    out.x = x - np.array(x).mean()\n",
    "    out.y = y - np.array(y).mean()\n",
    "    out.z = z - np.array(z).min()\n",
    "\n",
    "    global ground_counter, tree_counter\n",
    "    if folder == 'ground':\n",
    "        out.write(grounds_folder / f'ground_{ground_counter}.las')\n",
    "        ground_counter += 1\n",
    "    else:\n",
    "        out.write(trees_folder / f'tree_{tree_counter}.las')\n",
    "        tree_counter += 1\n",
    "\n",
    "def find_n_points(points, n):\n",
    "    if len(points) <= n:\n",
    "        return points\n",
    "    \n",
    "    choosen_idx = []\n",
    "    idx = np.arange(len(points))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = list(idx)\n",
    "\n",
    "    while len(choosen_idx) < n and len(idx) > 0:\n",
    "        candidate = idx.pop()\n",
    "        too_close = False\n",
    "        for c in choosen_idx:\n",
    "            dist = np.linalg.norm(points[c] - points[candidate])\n",
    "            if dist < MIN_DISTANCE:\n",
    "                too_close = True\n",
    "                break\n",
    "\n",
    "        if not too_close:\n",
    "            choosen_idx.append(candidate)\n",
    "\n",
    "    return points[np.array(choosen_idx)]\n",
    "\n",
    "def load_trees(paths: Path) -> List[LasData]:\n",
    "    trees = []\n",
    "    for path in paths:\n",
    "        with laspy.open(path) as f:\n",
    "            las_data = f.read()\n",
    "\n",
    "        trees.append(las_data)\n",
    "    return trees\n",
    "\n",
    "def chunkerize_four(file):\n",
    "    xy = np.stack([file.x, file.y], axis=1)\n",
    "    center = xy.mean(axis=0)\n",
    "\n",
    "    return [\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] < center[1]),\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] < center[1])\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b908b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:30<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    with laspy.open(file) as f:\n",
    "        ground = f.read()\n",
    "\n",
    "    ground_mask = ground.semantic_gt == 0\n",
    "    ground_points = ground.points[ground_mask]\n",
    "    save_las(ground, ground_points, 'ground')\n",
    "\n",
    "    unique_ids = np.unique(ground.instance_gt)\n",
    "    unique_ids = unique_ids[unique_ids > 0]\n",
    "\n",
    "    for id in unique_ids:\n",
    "        tree_mask = ground.instance_gt == id\n",
    "        tree_xyz = ground.points[tree_mask]\n",
    "        save_las(ground, tree_xyz, 'tree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fa62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:12<00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "ground_files = list((grounds_folder).glob('*.las'))\n",
    "tree_files = list((trees_folder).glob('*.las'))\n",
    "\n",
    "for file in tqdm(range(len(ground_files))):\n",
    "    with laspy.open(ground_files[file]) as f:\n",
    "        ground = f.read()\n",
    "\n",
    "    ground_scale = np.random.uniform(*SCALE_RANGE)\n",
    "    ground_yaw = np.radians(np.random.uniform(*YAW_RANGE))\n",
    "    ground_tilt = np.radians(np.random.uniform(*TILT_RANGE))\n",
    "\n",
    "    xyz = np.array(ground.xyz) * ground_scale\n",
    "    xyz = xyz @ np.array([\n",
    "        [np.cos(ground_yaw), -np.sin(ground_yaw), 0],\n",
    "        [np.sin(ground_yaw), np.cos(ground_yaw), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    xyz = xyz @ np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, np.cos(ground_tilt), -np.sin(ground_tilt)],\n",
    "        [0, np.sin(ground_tilt), np.cos(ground_tilt)]\n",
    "    ])\n",
    "\n",
    "    ground.xyz = xyz\n",
    "\n",
    "    for copy in range(AUGMENTED_PER_POINT_CLOUD):\n",
    "        num_trees = np.random.randint(MIN_TREES_PER_SCENE, MAX_TREES_PER_SCENE + 1)\n",
    "        choosen_points = find_n_points(xyz, num_trees)\n",
    "        choosen_trees = np.random.choice(tree_files, num_trees, replace=False)\n",
    "        trees = load_trees(choosen_trees)\n",
    "\n",
    "        out_path = raw_folder / 'aug' / f'{ground_files[file].stem}_aug_{copy}.las'\n",
    "        with laspy.open(out_path, mode='w', header=ground.header) as w:\n",
    "            w.write_points(ground.points)\n",
    "\n",
    "            for id, (point, tree) in enumerate(zip(choosen_points, trees)):\n",
    "                tree.instance_gt = np.full_like(tree.instance_gt, fill_value=id + 1)\n",
    "\n",
    "                tree_scale = np.random.uniform(*SCALE_RANGE)\n",
    "                tree_yaw = np.radians(np.random.uniform(*YAW_RANGE))\n",
    "                tree_tilt = np.radians(np.random.uniform(*TILT_RANGE))\n",
    "                \n",
    "                tree_xyz = np.array(tree.xyz) * tree_scale\n",
    "                tree_xyz = tree_xyz @ np.array([\n",
    "                    [np.cos(tree_yaw), -np.sin(tree_yaw), 0],\n",
    "                    [np.sin(tree_yaw), np.cos(tree_yaw), 0],\n",
    "                    [0, 0, 1]\n",
    "                ])\n",
    "                tree_xyz = tree_xyz @ np.array([\n",
    "                    [1, 0, 0],\n",
    "                    [0, np.cos(tree_tilt), -np.sin(tree_tilt)],\n",
    "                    [0, np.sin(tree_tilt), np.cos(tree_tilt)]\n",
    "                ])\n",
    "\n",
    "                tree_xyz += point\n",
    "                tree.xyz = tree_xyz\n",
    "\n",
    "                w.write_points(tree.points)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a905f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.45s/it]\n"
     ]
    }
   ],
   "source": [
    "folders = ['train', 'val', 'test', 'aug']\n",
    "for folder in tqdm(folders):\n",
    "    split_folder = raw_folder / folder\n",
    "    files = list((split_folder).glob('*.las'))\n",
    "\n",
    "    for file in files:\n",
    "        with laspy.open(file) as f:\n",
    "            las = f.read()\n",
    "\n",
    "        masks = chunkerize_four(las)\n",
    "        for i, mask in enumerate(masks):\n",
    "            chunk = las.points[mask]\n",
    "            out = laspy.create(point_format=las.point_format, file_version=las.header.version)\n",
    "            out.header.scales = las.header.scales\n",
    "            out.header.offsets = np.array([0.0, 0.0, 0.0])\n",
    "            out.points = chunk\n",
    "\n",
    "            x = chunk.x\n",
    "            y = chunk.y\n",
    "            z = chunk.z\n",
    "\n",
    "            out.x = x - np.array(x).min()\n",
    "            out.y = y - np.array(y).min()\n",
    "            out.z = z - np.array(z).min()\n",
    "\n",
    "            _, out.instance_gt = np.unique(chunk.instance_gt, return_inverse=True)\n",
    "\n",
    "            out_folder = (processed_folder / folder) if folder != 'aug' else (processed_folder / 'train')\n",
    "            out.write(out_folder / f'{file.stem}_chunk_{i}.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a63a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Archivos encontrados en train: 3268\n",
      "🎯 Archivos seleccionados para mover a val: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moviendo archivos a val: 100%|██████████| 32/32 [00:00<00:00, 14328.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Se movieron 32 archivos de train a val\n",
      "📊 Archivos restantes en train: 3236\n",
      "📊 Archivos totales en val: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_files = list((processed_folder / 'train').glob('*.las'))\n",
    "print(f\"📁 Archivos encontrados en train: {len(train_files)}\")\n",
    "\n",
    "if len(train_files) < VAL_CHUNKS:\n",
    "    print(f\"⚠️ Solo hay {len(train_files)} archivos en train, pero quieres mover {VAL_CHUNKS}\")\n",
    "    VAL_CHUNKS = len(train_files)\n",
    "    print(f\"📝 Ajustando a mover {VAL_CHUNKS} archivos\")\n",
    "\n",
    "selected_files = random.sample(train_files, VAL_CHUNKS)\n",
    "print(f\"🎯 Archivos seleccionados para mover a val: {len(selected_files)}\")\n",
    "\n",
    "moved_count = 0\n",
    "for file_path in tqdm(selected_files, desc=\"Moviendo archivos a val\"):\n",
    "    try:\n",
    "        destination = processed_folder / 'val' / file_path.name\n",
    "        shutil.move(str(file_path), str(destination))\n",
    "        moved_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error moviendo {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"✅ Se movieron {moved_count} archivos de train a val\")\n",
    "print(f\"📊 Archivos restantes en train: {len(list((processed_folder / 'train').glob('*.las')))}\")\n",
    "print(f\"📊 Archivos totales en val: {len(list((processed_folder / 'val').glob('*.las')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc183f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando 825 archivos en la carpeta 'aug'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking aug files: 100%|██████████| 825/825 [00:00<00:00, 847.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 RESUMEN GLOBAL:\n",
      "IDs únicos encontrados en todo el conjunto 'aug': [0, 1, 2]\n",
      "Rango de IDs: 0 - 2\n",
      "Total de IDs únicos diferentes: 3\n",
      "\n",
      "📋 DETALLES POR ARCHIVO (primeros 10):\n",
      "Ground_12_2025-10-13_17h24_08_872_aug_2.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_8_2025-10-13_17h39_56_233_aug_14.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_15_2025-10-13_17h36_53_125_aug_14.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_4_2025-10-13_17h36_52_815_aug_8.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_18_2025-10-13_17h36_53_204_aug_5.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_28_2025-10-13_17h36_53_468_aug_6.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_8_2025-10-13_17h36_52_923_aug_11.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_7_2025-10-13_17h24_08_744_aug_10.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_14_2025-10-13_17h36_53_100_aug_6.las: IDs [0 1 2] (rango: 0-2, total: 3)\n",
      "Ground_5_2025-10-13_17h39_56_117_aug_2.las: IDs [0 1 2] (rango: 0-2, total: 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar IDs únicos en todas las nubes de aug\n",
    "aug_folder = raw_folder / 'aug'\n",
    "aug_files = list(aug_folder.glob('*.las'))\n",
    "\n",
    "print(f\"Analizando {len(aug_files)} archivos en la carpeta 'aug'...\")\n",
    "\n",
    "all_unique_ids = set()\n",
    "file_id_info = []\n",
    "\n",
    "for aug_file in tqdm(aug_files, desc=\"Checking aug files\"):\n",
    "    with laspy.open(aug_file) as f:\n",
    "        las = f.read()\n",
    "    \n",
    "    unique_ids = np.unique(las.semantic_gt)\n",
    "    all_unique_ids.update(unique_ids)\n",
    "    \n",
    "    file_id_info.append({\n",
    "        'file': aug_file.name,\n",
    "        'unique_ids': unique_ids,\n",
    "        'min_id': np.min(unique_ids),\n",
    "        'max_id': np.max(unique_ids),\n",
    "        'num_unique': len(unique_ids)\n",
    "    })\n",
    "\n",
    "print(f\"\\n📊 RESUMEN GLOBAL:\")\n",
    "print(f\"IDs únicos encontrados en todo el conjunto 'aug': {sorted(all_unique_ids)}\")\n",
    "print(f\"Rango de IDs: {min(all_unique_ids)} - {max(all_unique_ids)}\")\n",
    "print(f\"Total de IDs únicos diferentes: {len(all_unique_ids)}\")\n",
    "\n",
    "print(f\"\\n📋 DETALLES POR ARCHIVO (primeros 10):\")\n",
    "for i, info in enumerate(file_id_info[:10]):\n",
    "    print(f\"{info['file']}: IDs {info['unique_ids']} (rango: {info['min_id']}-{info['max_id']}, total: {info['num_unique']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
