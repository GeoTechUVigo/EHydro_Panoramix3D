{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchsparse\n",
    "import hdbscan\n",
    "import laspy\n",
    "import time\n",
    "\n",
    "from EHydro_TreeUnet.tree_unet import UNet\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import nn\n",
    "from torch.cuda import amp\n",
    "from torchsparse import SparseTensor\n",
    "from torchsparse.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "from torchsparse.utils.collate import sparse_collate_fn as _orig_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FORinstanceDataset:\n",
    "    def __init__(self, voxel_size: float) -> None:\n",
    "        self.voxel_size = voxel_size\n",
    "        self.folder = Path('./datasets/FORinstance')\n",
    "        self.extensions = ('.laz', '.las')\n",
    "        self.files = sorted(\n",
    "            [f for f in self.folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in self.extensions],\n",
    "            key=lambda f: f.name\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return [self._load_file(self.files[i]) for i in range(*idx.indices(len(self)))]\n",
    "        elif isinstance(idx, int):\n",
    "            if idx < 0:\n",
    "                idx += len(self)\n",
    "            if idx < 0 or idx >= len(self):\n",
    "                raise IndexError(\"Index out of range\")\n",
    "            return self._load_file(self.files[idx])\n",
    "        else:\n",
    "            raise TypeError(\"Index must be a slice or an integer\")\n",
    "    \n",
    "    def _load_file(self, path):\n",
    "        las = laspy.read(path)\n",
    "\n",
    "        coords = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "        coords -= np.min(coords, axis=0, keepdims=True)\n",
    "        feat = np.vstack((las.intensity, las.return_number, las.number_of_returns)).transpose()\n",
    "        label = np.array(las.classification)\n",
    "        instance_ids = np.array(las.treeID)\n",
    "\n",
    "        mask = label != 3\n",
    "\n",
    "        coords = coords[mask]\n",
    "        feat = feat[mask]\n",
    "        label = label[mask]\n",
    "        label = np.where(label > 3, label - 1, label)\n",
    "        instance_ids = instance_ids[mask]\n",
    "            \n",
    "        coords, indices = sparse_quantize(coords, self.voxel_size, return_index=True)\n",
    "        feat = feat[indices]\n",
    "        label = label[indices]\n",
    "        instance_ids = instance_ids[indices]\n",
    "\n",
    "        offset = np.zeros((coords.shape[0], 3), dtype=np.float32)\n",
    "        unique_ids = np.unique(instance_ids)\n",
    "        for inst_id in unique_ids:\n",
    "            mask = instance_ids == inst_id\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            tree_points = coords[mask]\n",
    "            offset[mask, :] = tree_points.mean(axis=0) - tree_points\n",
    "\n",
    "        #min_coords = np.min(coords, axis=0)\n",
    "        #max_coords = np.max(coords, axis=0)\n",
    "        #side_lengths = max_coords - min_coords\n",
    "        #volume = np.prod(side_lengths)\n",
    "        #offset /= volume\n",
    "\n",
    "        coords = torch.tensor(coords, dtype=torch.int)\n",
    "        feat = torch.tensor(feat.astype(np.float32), dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        offset = torch.tensor(offset, dtype=torch.float)\n",
    "\n",
    "        input = SparseTensor(coords=coords, feats=feat)\n",
    "        label = SparseTensor(coords=coords, feats=label)\n",
    "        offset = SparseTensor(coords=coords, feats=offset)\n",
    "        return {\"input\": input, \"label\": label, \"offset\": offset}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\n",
    "    [255, 0, 0],    # clase 0 - Unclassified - rojo\n",
    "    [0, 255, 0],    # clase 1 - Low-vegetation - verde\n",
    "    [128, 128, 128],# clase 2 - Terrain - gris\n",
    "    [255, 165, 0],  # clase 3 - Stem - naranja\n",
    "    [0, 128, 0],  # clase 4 - Live-branches - verde oscuro\n",
    "    [0, 0, 255]     # clase 5 - Woody-branches - azul\n",
    "], dtype=np.uint8)\n",
    "\n",
    "def draw_pc(coords, labels, ids):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    colors = colormap[labels]\n",
    "    colors = colors / 255.0\n",
    "    \n",
    "    pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    unique_ids = np.unique(ids)\n",
    "    n_instances = len(unique_ids)\n",
    "\n",
    "    cmap = plt.get_cmap(\"tab20\")  # Puedes cambiar a 'tab10', 'gist_ncar', etc.\n",
    "    colors = np.array([cmap(i % 20)[:3] for i in range(n_instances)])\n",
    "\n",
    "    id2color = {id_: colors[i] for i, id_ in enumerate(unique_ids)}\n",
    "    point_colors = np.array([id2color[id_] for id_ in ids])\n",
    "\n",
    "    pcd.colors = o3d.utility.Vector3dVector(point_colors)\n",
    "    o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 1] loss = 10.772480964660645\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[27, 64, 192]' is invalid for input of size 276480",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     39\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/function.py:87\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchsparse-2.1.0-py3.8-linux-x86_64.egg/torchsparse/nn/functional/conv/func/implicit_gemm.py:146\u001b[0m, in \u001b[0;36mImplicitGEMMConvolutionFuntion.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    132\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m torchsparse\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mconv_forward_implicit_gemm_sorted_cuda(\n\u001b[1;32m    133\u001b[0m         grad_output,\n\u001b[1;32m    134\u001b[0m         weight\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m         torchsparse\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mallow_fp16,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# wgrad\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     grad_weight \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 146\u001b[0m         \u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorchsparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_backward_wgrad_implicit_gemm_sorted_cuda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreorder_out_in_map_bwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreduced_sorted_mask_bwd_wgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreorder_loc_bwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtorchsparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_tf32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtorchsparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_volume\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# unsort mode\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# dgrad\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m torchsparse\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mconv_forward_implicit_gemm_cuda(\n\u001b[1;32m    166\u001b[0m         grad_output,\n\u001b[1;32m    167\u001b[0m         weight\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m         torchsparse\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mallow_fp16,\n\u001b[1;32m    173\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[27, 64, 192]' is invalid for input of size 276480"
     ]
    }
   ],
   "source": [
    "conv_config = F.conv_config.get_default_conv_config(conv_mode=F.get_conv_mode())\n",
    "conv_config.kmap_mode = 'hashmap'\n",
    "F.conv_config.set_global_conv_config(conv_config)\n",
    "\n",
    "model = UNet(3, 6).to(device='cuda')\n",
    "\n",
    "dataset = FORinstanceDataset(voxel_size=0.2)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sparse_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=sparse_collate_fn)\n",
    "\n",
    "criterion_semantic = nn.CrossEntropyLoss()\n",
    "criterion_offset = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scaler = amp.GradScaler(enabled=True)\n",
    "\n",
    "for k, feed_dict in enumerate(train_loader):\n",
    "    inputs = feed_dict[\"input\"].to(device='cuda')\n",
    "    label = feed_dict[\"label\"].to(device='cuda')\n",
    "    offset = feed_dict[\"offset\"].to(device='cuda')\n",
    "\n",
    "    with amp.autocast(enabled=True):\n",
    "        semantic_output, offset_output = model(inputs)\n",
    "        \n",
    "        loss_semantic = criterion_semantic(semantic_output.feats, label.feats)\n",
    "        loss_offset = criterion_offset(offset_output.feats, offset.feats)\n",
    "        loss = 0.1 * loss_semantic + 0.9 * loss_offset\n",
    "\n",
    "    print(f\"[step {k + 1}] loss = {loss.item()}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7437  0.1825  4.305 ]\n",
      " [ 1.154   0.4395  4.074 ]\n",
      " [ 0.706  -0.6387  4.75  ]\n",
      " ...\n",
      " [-0.4534 -0.1288  2.275 ]\n",
      " [-0.857  -0.5356  2.559 ]\n",
      " [-1.559  -0.4438  3.717 ]]\n",
      "[inference step 1] loss = 10.033637046813965\n",
      "[[ 0.2091 -0.1625  0.4011]\n",
      " [ 0.2119 -0.1364  0.2485]\n",
      " [ 0.405  -0.1638  0.3062]\n",
      " ...\n",
      " [-0.288  -0.3105  0.578 ]\n",
      " [-0.4766 -0.3914  0.9814]\n",
      " [-0.7026 -0.37    1.198 ]]\n",
      "[inference step 2] loss = 9.115703582763672\n",
      "[[ 0.371  -0.2546  0.6567]\n",
      " [ 0.7065 -0.1864  0.9097]\n",
      " [ 0.2449 -0.2338  0.3481]\n",
      " ...\n",
      " [-0.873  -0.4504  1.921 ]\n",
      " [-0.893  -0.6753  2.    ]\n",
      " [-0.5645 -0.7246  1.573 ]]\n",
      "[inference step 3] loss = 8.601606369018555\n"
     ]
    }
   ],
   "source": [
    "# enable torchsparse 2.0 inference\n",
    "model.eval()\n",
    "# enable fused and locality-aware memory access optimization\n",
    "torchsparse.backends.benchmark = True  # type: ignore\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "with torch.no_grad():\n",
    "    for k, feed_dict in enumerate(val_loader):\n",
    "        inputs = feed_dict[\"input\"].to(device='cuda')\n",
    "        label = feed_dict[\"label\"].to(device='cuda')\n",
    "        offset = feed_dict[\"offset\"].to(device='cuda')\n",
    "\n",
    "        with amp.autocast(enabled=True):\n",
    "            now = time.time()\n",
    "            semantic_output, offset_output = model(inputs)\n",
    "            print(f'duracion: {(time.time() - now):.2f} s')\n",
    "\n",
    "            loss_semantic = criterion_semantic(semantic_output.feats, label.feats)\n",
    "            loss_offset = criterion_offset(offset_output.feats, offset.feats)\n",
    "            loss = 0.1 * loss_semantic + 0.9 * loss_offset\n",
    "\n",
    "            coords = semantic_output.coords[:, 1:].cpu().numpy()\n",
    "            semantic = semantic_output.feats.cpu()\n",
    "            offset = offset_output.feats.cpu().numpy()\n",
    "            label = torch.argmax(semantic, dim=1).numpy()\n",
    "\n",
    "            #min_coords = np.min(coords, axis=0)\n",
    "            #max_coords = np.max(coords, axis=0)\n",
    "            #side_lengths = max_coords - min_coords\n",
    "            #volume = np.prod(side_lengths)\n",
    "\n",
    "            print(offset)\n",
    "\n",
    "            collapsed_coords = coords + offset\n",
    "            id = clusterer.fit_predict(collapsed_coords)\n",
    "\n",
    "            draw_pc(coords, label, id)\n",
    "\n",
    "        print(f\"[inference step {k + 1}] loss = {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
