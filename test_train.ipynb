{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XDG_SESSION_TYPE\"] = \"x11\"\n",
    "os.environ.pop(\"WAYLAND_DISPLAY\", None)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from EHydro_TreeUnet.trainers import TreeProjectorTrainer\n",
    "from torchsparse.nn import functional as F\n",
    "\n",
    "F.set_kmap_mode(\"hashmap_on_the_fly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "TREE_PROJECTOR_DIR = Path(os.environ.get('TREE_PROJECTOR_DIR', Path.home() / 'tree_projector'))\n",
    "DATASET_FOLDER = 'MixedDataset'\n",
    "VERSION_NAME = 'tree_projector_VS-0.2_DA-48_E-3_V3'\n",
    "\n",
    "VOXEL_SIZE = 0.2\n",
    "FEAT_KEYS = ['intensity']\n",
    "# FEAT_KEYS = ['intensity']\n",
    "CENTROID_SIGMA = 1.5\n",
    "TRAIN_PCT = 0.9\n",
    "DATA_AUGMENTATION_COEF = 48\n",
    "YAW_RANGE = (0.0, 360.0)\n",
    "TILT_RANGE = (-5.0, 5.0)\n",
    "SCALE_RANGE = (0.9, 1.3)\n",
    "\n",
    "TRAINING = True\n",
    "TEST_LR = []\n",
    "EPOCHS = 2\n",
    "START_ON_EPOCH = 0\n",
    "BATCH_SIZE = 1\n",
    "SEMANTIC_LOSS_COEF = 2.0\n",
    "CENTROID_LOSS_COEF = 1.0\n",
    "OFFSET_LOSS_COEF = 4.0\n",
    "INSTANCE_LOSS_COEF = 1.0\n",
    "BACKBONE_LR = 2e-3\n",
    "SEMANTIC_LR = 1e-3\n",
    "OFFSET_LR = 1e-3\n",
    "CENTROID_LR = 1e-3\n",
    "INSTANCE_LR = 1e-3\n",
    "WEIGHT_DECAY = 0.04\n",
    "\n",
    "RESNET_BLOCKS = [\n",
    "    (3, 16, 3, 1),\n",
    "    (3, 32, 3, 2),\n",
    "    (3, 64, 3, 2),\n",
    "    (3, 128, 3, 2),\n",
    "    (1, 128, (1, 1, 3), (1, 1, 2)),\n",
    "]\n",
    "INSTANCE_DENSITY = 0.01\n",
    "SCORE_THRES = 0.1\n",
    "CENTROID_THRES = 0.15\n",
    "DESCRIPTOR_DIM = 16\n",
    "\n",
    "CHARTS_IGNORE_CLASS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(arr: np.ndarray, window: int) -> np.ndarray:\n",
    "    if window <= 1:\n",
    "        return arr\n",
    "\n",
    "    kernel = np.ones(window, dtype=float)\n",
    "    if arr.ndim == 1:\n",
    "        denom = np.convolve(np.ones_like(arr), kernel, mode=\"same\")\n",
    "        return np.convolve(arr, kernel, mode=\"same\") / denom\n",
    "\n",
    "    # 2-D: suavizar cada columna por separado\n",
    "    smoothed = np.empty_like(arr, dtype=float)\n",
    "    denom = np.convolve(np.ones(arr.shape[0]), kernel, mode=\"same\")\n",
    "    for c in range(arr.shape[1]):\n",
    "        smoothed[:, c] = np.convolve(arr[:, c], kernel, mode=\"same\") / denom\n",
    "    return smoothed\n",
    "        \n",
    "def gen_charts(trainer, losses, stats, training: bool, window: int = 1, ignore_class = []):\n",
    "    keys = stats[0].keys()\n",
    "    stats = {k: np.array([d[k] for d in stats]) for k in keys}\n",
    "\n",
    "    loss = np.asarray(losses)\n",
    "    loss_s = np.clip(smooth(loss[:, 0], window), 0.0, 10.0)\n",
    "    loss_sem_s = np.clip(smooth(loss[:, 1], window), 0.0, 10.0)\n",
    "    loss_centroid_s = np.clip(smooth(loss[:, 2], window), 0.0, 10.0)\n",
    "    loss_inst_s = np.clip(smooth(loss[:, 3], window), 0.0, 10.0)\n",
    "\n",
    "    iou_semantic = smooth(stats['iou_semantic'], window)\n",
    "    mean_iou_semantic = smooth(stats['mean_iou_semantic'], window)\n",
    "    precision_semantic = smooth(stats['precision_semantic'], window)\n",
    "    mean_precision_semantic = smooth(stats['mean_precision_semantic'], window)\n",
    "    recall_semantic = smooth(stats['recall_semantic'], window)\n",
    "    mean_recall_semantic = smooth(stats['mean_recall_semantic'], window)\n",
    "    f1_semantic = smooth(stats['f1_semantic'], window)\n",
    "    mean_f1_semantic = smooth(stats['mean_f1_semantic'], window)\n",
    "\n",
    "    mean_iou_instance = smooth(stats['mean_iou_instance'], window)\n",
    "\n",
    "    # --- Global loss -----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Semantic loss ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_sem_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Semantic loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Centroid loss ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_centroid_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Centroid loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Instance loss ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_inst_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Instance loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Semantic IoU -----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for c in range(trainer.dataset.num_classes):\n",
    "        if trainer.dataset.class_names[c] in ignore_class:\n",
    "            continue\n",
    "        \n",
    "        plt.plot(iou_semantic[:, c], label=trainer.dataset.class_names[c])\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"IoU\")\n",
    "    plt.title(f\"Semantic IoU evolution during {'Training' if training else 'Inference'} (MA{window})\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Semantic mIoU ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(mean_iou_semantic, label=f\"{'Training' if training else 'Inference'} mIoU (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"mIoU\")\n",
    "    plt.title(f\"Semantic mIoU evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Instance mIoU ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(mean_iou_instance, label=f\"{'Training' if training else 'Inference'} Instance mIoU (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"mIoU\")\n",
    "    plt.title(f\"Instance mIoU evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    column_names = ['IoU', 'Precision', 'Recall', 'F1']\n",
    "    row_names = [trainer.dataset.class_names[c] for c in range(trainer.dataset.num_classes) if trainer.dataset.class_names[c] not in ignore_class]\n",
    "    row_names.append('Mean')\n",
    "\n",
    "    iou_semantic_arr = stats['iou_semantic']\n",
    "    prec_arr = stats['precision_semantic']\n",
    "    recall_arr = stats['recall_semantic']\n",
    "    f1_arr = stats['f1_semantic']\n",
    "\n",
    "    data = [\n",
    "        [iou_semantic_arr[:, c].mean(), prec_arr[:, c].mean(), recall_arr[:, c].mean(), f1_arr[:, c].mean()]\n",
    "    for c in range(trainer.dataset.num_classes) if trainer.dataset.class_names[c] not in ignore_class]\n",
    "\n",
    "    means = np.array(data).mean(axis=0)\n",
    "    data.append(list(means))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names, index=row_names)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros totales: 16,000,680\n",
      "Parámetros entrenables: 16,000,680\n",
      "Resnet generates features at the following scales:\n",
      "\t* (0.2, 0.2, 0.2) meters -> 16 feats.\n",
      "\t* (0.4, 0.4, 0.4) meters -> 32 feats.\n",
      "\t* (0.8, 0.8, 0.8) meters -> 64 feats.\n",
      "\t* (1.6, 1.6, 1.6) meters -> 128 feats.\n",
      "\t* (1.6, 1.6, 3.2) meters -> 128 feats.\n",
      "\n",
      "Minimum scene size: (4.8, 4.8, 9.6) meters\n",
      "Version name: tree_projector_VS-0.2_DA-48_E-3_V3\n",
      "\n",
      "=== Starting epoch 1 ===\n",
      "[Train]: 100%|██████████| 5520/5520 [23:43<00:00,  3.88it/s, VRAM=2.62 GB, Matched=10 / 20, TP=0 / 20, Total=4.0107 ↓, Semantic=0.2370 ↓, Centroid=1.7505 ↑, Offset=0.7215 ↓, Instance=1.3018 ↓]  \n",
      "\n",
      "✅ Epoch 1 finished.\n",
      "Version name: tree_projector_VS-0.2_DA-48_E-3_V3\n",
      "\n",
      "=== Starting epoch 2 ===\n",
      "[Train]: 100%|██████████| 5520/5520 [23:44<00:00,  3.88it/s, VRAM=2.75 GB, Matched=10 / 9, TP=1 / 9, Total=3.2517 ↑, Semantic=0.1878 ↑, Centroid=1.3141 ↑, Offset=0.4499 ↓, Instance=1.2998 →]    \n",
      "\n",
      "✅ Epoch 2 finished.\n",
      "Version name: tree_projector_VS-0.2_DA-48_E-3_V3\n",
      "\n",
      "=== Starting epoch 3 ===\n",
      "[Train]: 100%|██████████| 5520/5520 [23:57<00:00,  3.84it/s, VRAM=2.75 GB, Matched=13 / 10, TP=1 / 10, Total=4.2443 ↓, Semantic=0.0896 ↓, Centroid=2.0038 ↓, Offset=0.9849 ↓, Instance=1.1660 ↑]  \n",
      "\n",
      "✅ Epoch 3 finished.\n",
      "Version name: tree_projector_VS-0.2_DA-48_E-3_V3\n",
      "\n",
      "=== Starting epoch 4 ===\n",
      "[Train]: 100%|██████████| 5520/5520 [24:09<00:00,  3.81it/s, VRAM=2.76 GB, Matched=15 / 13, TP=2 / 13, Total=2.8116 ↓, Semantic=0.1531 →, Centroid=1.0368 ↑, Offset=0.4216 ↓, Instance=1.2002 ↓]  \n",
      "\n",
      "✅ Epoch 4 finished.\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "[Val]:   0%|          | 0/13 [00:00<?, ?it/s]\n",
      "[Val]:   0%|          | 0/13 [00:00<?, ?it/s, VRAM=2.76 GB, Matched=15 / 12, TP=0 / 12, Total=6.9125 →, Semantic=0.8854 →, Centroid=2.5852 →, Offset=1.2056 →, Instance=2.2363 →]\n",
      "[Val]:   8%|▊         | 1/13 [00:15<03:10, 15.85s/it, VRAM=2.76 GB, Matched=15 / 12, TP=0 / 12, Total=6.9125 →, Semantic=0.8854 →, Centroid=2.5852 →, Offset=1.2056 →, Instance=2.2363 →]\n",
      "[Val]:   8%|▊         | 1/13 [00:16<03:10, 15.85s/it, VRAM=2.76 GB, Matched=13 / 15, TP=0 / 15, Total=7.3260 ↑, Semantic=0.4952 ↓, Centroid=2.6928 ↑, Offset=1.4583 ↑, Instance=2.6797 ↑]\n",
      "[Val]:  15%|█▌        | 2/13 [00:21<01:45,  9.64s/it, VRAM=2.76 GB, Matched=13 / 15, TP=0 / 15, Total=7.3260 ↑, Semantic=0.4952 ↓, Centroid=2.6928 ↑, Offset=1.4583 ↑, Instance=2.6797 ↑]\n",
      "[Val]:  15%|█▌        | 2/13 [00:21<01:45,  9.64s/it, VRAM=2.76 GB, Matched=19 / 5, TP=1 / 5, Total=10.9727 ↑, Semantic=0.0895 ↓, Centroid=4.4918 ↑, Offset=4.8895 ↑, Instance=1.5020 ↓]\n",
      "[Val]:  23%|██▎       | 3/13 [00:25<01:12,  7.24s/it, VRAM=2.76 GB, Matched=19 / 5, TP=1 / 5, Total=10.9727 ↑, Semantic=0.0895 ↓, Centroid=4.4918 ↑, Offset=4.8895 ↑, Instance=1.5020 ↓]\n",
      "[Val]:  23%|██▎       | 3/13 [00:25<01:12,  7.24s/it, VRAM=2.76 GB, Matched=13 / 8, TP=1 / 8, Total=7.6769 ↑, Semantic=0.6057 ↓, Centroid=2.9811 ↑, Offset=1.2210 ↑, Instance=2.8691 ↑]\n",
      "[Val]:  31%|███       | 4/13 [00:28<00:51,  5.74s/it, VRAM=2.76 GB, Matched=13 / 8, TP=1 / 8, Total=7.6769 ↑, Semantic=0.6057 ↓, Centroid=2.9811 ↑, Offset=1.2210 ↑, Instance=2.8691 ↑]\n",
      "[Val]:  31%|███       | 4/13 [00:29<00:51,  5.74s/it, VRAM=2.76 GB, Matched=19 / 19, TP=0 / 19, Total=8.7127 ↑, Semantic=0.2508 ↓, Centroid=3.6347 ↑, Offset=2.8272 ↑, Instance=2.0000 ↓]\n",
      "[Val]:  38%|███▊      | 5/13 [00:33<00:41,  5.23s/it, VRAM=2.76 GB, Matched=19 / 19, TP=0 / 19, Total=8.7127 ↑, Semantic=0.2508 ↓, Centroid=3.6347 ↑, Offset=2.8272 ↑, Instance=2.0000 ↓]\n",
      "[Val]:  38%|███▊      | 5/13 [00:33<00:41,  5.23s/it, VRAM=2.76 GB, Matched=14 / 30, TP=0 / 30, Total=9.7671 ↑, Semantic=0.3932 ↓, Centroid=2.9495 ↑, Offset=1.7331 ↑, Instance=4.6914 ↑]\n",
      "[Val]:  46%|████▌     | 6/13 [00:36<00:32,  4.68s/it, VRAM=2.76 GB, Matched=14 / 30, TP=0 / 30, Total=9.7671 ↑, Semantic=0.3932 ↓, Centroid=2.9495 ↑, Offset=1.7331 ↑, Instance=4.6914 ↑]\n",
      "[Val]:  46%|████▌     | 6/13 [00:36<00:32,  4.68s/it, VRAM=2.76 GB, Matched=16 / 17, TP=0 / 17, Total=8.2420 ↑, Semantic=0.3484 ↓, Centroid=3.0026 ↑, Offset=1.8715 ↑, Instance=3.0195 ↑]\n",
      "[Val]:  54%|█████▍    | 7/13 [00:40<00:26,  4.48s/it, VRAM=2.76 GB, Matched=16 / 17, TP=0 / 17, Total=8.2420 ↑, Semantic=0.3484 ↓, Centroid=3.0026 ↑, Offset=1.8715 ↑, Instance=3.0195 ↑]\n",
      "[Val]:  54%|█████▍    | 7/13 [00:41<00:26,  4.48s/it, VRAM=2.76 GB, Matched=13 / 6, TP=0 / 6, Total=8.0556 ↑, Semantic=0.3191 ↓, Centroid=2.4017 ↓, Offset=0.8543 ↓, Instance=4.4805 ↑]\n",
      "[Val]:  62%|██████▏   | 8/13 [00:44<00:20,  4.14s/it, VRAM=2.76 GB, Matched=13 / 6, TP=0 / 6, Total=8.0556 ↑, Semantic=0.3191 ↓, Centroid=2.4017 ↓, Offset=0.8543 ↓, Instance=4.4805 ↑]\n",
      "[Val]:  62%|██████▏   | 8/13 [00:44<00:20,  4.14s/it, VRAM=2.76 GB, Matched=13 / 4, TP=1 / 4, Total=12.4614 ↑, Semantic=0.3540 ↓, Centroid=6.0516 ↑, Offset=4.6905 ↑, Instance=1.3652 ↑]\n",
      "[Val]:  69%|██████▉   | 9/13 [00:48<00:16,  4.16s/it, VRAM=2.76 GB, Matched=13 / 4, TP=1 / 4, Total=12.4614 ↑, Semantic=0.3540 ↓, Centroid=6.0516 ↑, Offset=4.6905 ↑, Instance=1.3652 ↑]\n",
      "[Val]:  69%|██████▉   | 9/13 [00:48<00:16,  4.16s/it, VRAM=2.76 GB, Matched=17 / 15, TP=1 / 15, Total=7.3420 ↑, Semantic=0.5221 ↓, Centroid=3.3600 ↑, Offset=1.9853 ↑, Instance=1.4746 ↑]\n",
      "[Val]:  77%|███████▋  | 10/13 [00:52<00:12,  4.17s/it, VRAM=2.76 GB, Matched=17 / 15, TP=1 / 15, Total=7.3420 ↑, Semantic=0.5221 ↓, Centroid=3.3600 ↑, Offset=1.9853 ↑, Instance=1.4746 ↑]\n",
      "[Val]:  77%|███████▋  | 10/13 [00:52<00:12,  4.17s/it, VRAM=2.76 GB, Matched=19 / 31, TP=0 / 31, Total=8.2278 ↑, Semantic=0.4140 ↓, Centroid=2.7522 ↑, Offset=1.8605 ↑, Instance=3.2012 ↑]\n",
      "[Val]:  85%|████████▍ | 11/13 [00:56<00:08,  4.01s/it, VRAM=2.76 GB, Matched=19 / 31, TP=0 / 31, Total=8.2278 ↑, Semantic=0.4140 ↓, Centroid=2.7522 ↑, Offset=1.8605 ↑, Instance=3.2012 ↑]\n",
      "[Val]:  85%|████████▍ | 11/13 [00:56<00:08,  4.01s/it, VRAM=2.76 GB, Matched=11 / 11, TP=1 / 11, Total=7.4928 ↑, Semantic=0.3165 ↓, Centroid=2.7736 ↑, Offset=2.1058 ↑, Instance=2.2969 ↑]\n",
      "[Val]:  92%|█████████▏| 12/13 [00:59<00:03,  3.79s/it, VRAM=2.76 GB, Matched=11 / 11, TP=1 / 11, Total=7.4928 ↑, Semantic=0.3165 ↓, Centroid=2.7736 ↑, Offset=2.1058 ↑, Instance=2.2969 ↑]\n",
      "[Val]:  92%|█████████▏| 12/13 [00:59<00:03,  3.79s/it, VRAM=2.76 GB, Matched=17 / 28, TP=0 / 28, Total=7.6881 ↓, Semantic=0.4463 ↓, Centroid=2.5640 ↓, Offset=2.0060 ↑, Instance=2.6719 ↑]\n",
      "[Val]: 100%|██████████| 13/13 [01:03<00:00,  3.73s/it, VRAM=2.76 GB, Matched=17 / 28, TP=0 / 28, Total=7.6881 ↓, Semantic=0.4463 ↓, Centroid=2.5640 ↓, Offset=2.0060 ↑, Instance=2.6719 ↑]\n",
      "[Val]: 100%|██████████| 13/13 [01:03<00:00,  4.87s/it, VRAM=2.76 GB, Matched=17 / 28, TP=0 / 28, Total=7.6881 ↓, Semantic=0.4463 ↓, Centroid=2.5640 ↓, Offset=2.0060 ↑, Instance=2.6719 ↑]\n",
      ", VRAM=2.76 GB, Matched=17 / 28, TP=0 / 28, Total=7.6881 ↓, Semantic=0.4463 ↓, Centroid=2.5640 ↓, Offset=2.0060 ↑, Instance=2.6719 ↑\n"
     ]
    }
   ],
   "source": [
    "trainer = TreeProjectorTrainer(\n",
    "    tree_projector_dir=TREE_PROJECTOR_DIR,\n",
    "    dataset_folder=DATASET_FOLDER,\n",
    "    version_name=VERSION_NAME,\n",
    "\n",
    "    voxel_size=VOXEL_SIZE,\n",
    "    feat_keys=FEAT_KEYS,\n",
    "    centroid_sigma=CENTROID_SIGMA,\n",
    "    train_pct=TRAIN_PCT,\n",
    "    data_augmentation_coef=DATA_AUGMENTATION_COEF,\n",
    "    yaw_range=YAW_RANGE,\n",
    "    tilt_range=TILT_RANGE,\n",
    "    scale_range=SCALE_RANGE,\n",
    "\n",
    "    training=TRAINING,\n",
    "    test_lr=TEST_LR,\n",
    "    epochs=EPOCHS,\n",
    "    start_on_epoch=START_ON_EPOCH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    semantic_loss_coef=SEMANTIC_LOSS_COEF,\n",
    "    centroid_loss_coef=CENTROID_LOSS_COEF,\n",
    "    offset_loss_coef=OFFSET_LOSS_COEF,\n",
    "    instance_loss_coef=INSTANCE_LOSS_COEF,\n",
    "    backbone_lr=BACKBONE_LR,\n",
    "    semantic_lr=SEMANTIC_LR,\n",
    "    offset_lr=OFFSET_LR,\n",
    "    centroid_lr=CENTROID_LR,\n",
    "    instance_lr=INSTANCE_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "\n",
    "    resnet_blocks=RESNET_BLOCKS,\n",
    "    instance_density=INSTANCE_DENSITY,\n",
    "    score_thres=SCORE_THRES,\n",
    "    centroid_thres=CENTROID_THRES,\n",
    "    descriptor_dim=DESCRIPTOR_DIM\n",
    ")\n",
    "\n",
    "if TRAINING:\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pcd_gt = o3d.geometry.PointCloud()\n",
    "pcd_pred = o3d.geometry.PointCloud()\n",
    "\n",
    "for eval_result in trainer.eval():\n",
    "    batch_idx = eval_result['semantic_output'].C.cpu().numpy()[:, 0]\n",
    "    centroid_batch_idx = eval_result['centroid_confidence_output'].C.cpu().numpy()[:, 0]\n",
    "\n",
    "    voxels = eval_result['semantic_output'].C.cpu().numpy()[:, 1:]\n",
    "    centroid_voxels = eval_result['centroid_confidence_output'].C.cpu().numpy()[:, 1:]\n",
    "\n",
    "    semantic_output = torch.argmax(eval_result['semantic_output'].F.cpu(), dim=1).numpy()\n",
    "    semantic_labels = eval_result['semantic_labels'].F.cpu().numpy()\n",
    "\n",
    "    semantic_mask = semantic_labels != 0\n",
    "    centroid_score_output = np.zeros((voxels.shape[0], 1), dtype=float)\n",
    "    centroid_score_output[semantic_mask] = eval_result['centroid_score_output'].F.cpu().numpy()\n",
    "    centroid_score_labels = np.zeros((voxels.shape[0], 1), dtype=float)\n",
    "    centroid_score_labels[semantic_mask] = eval_result['centroid_score_labels'].F.cpu().numpy()\n",
    "\n",
    "    centroid_confidence_output = eval_result['centroid_confidence_output'].F.cpu().numpy()\n",
    "\n",
    "    offset_output = np.zeros((voxels.shape[0], 3), dtype=float)\n",
    "    offset_output[semantic_mask] = eval_result['offset_output'].F.cpu().numpy()\n",
    "    offset_output = np.sign(offset_output) * np.expm1(np.abs(offset_output))\n",
    "    offset_labels = np.zeros((voxels.shape[0], 3), dtype=float)\n",
    "    offset_labels[semantic_mask] = eval_result['offset_labels'].F.cpu().numpy()\n",
    "    offset_labels = np.sign(offset_labels) * np.expm1(np.abs(offset_labels))\n",
    "\n",
    "    instance_output_remap_logits = torch.zeros((eval_result['instance_output'].F.size(0), eval_result['remap_info']['num_instances']), device=eval_result['instance_output'].F.device, dtype=eval_result['instance_output'].F.dtype)\n",
    "    instance_output_remap_logits[:, eval_result['remap_info']['gt_indices']] = eval_result['instance_output'].F[:, eval_result['remap_info']['pred_indices']]\n",
    "\n",
    "    instance_output = np.zeros((voxels.shape[0],), dtype=int)\n",
    "    instance_output[semantic_mask] = torch.argmax(eval_result['instance_output'].F.cpu(), dim=1).numpy()\n",
    "    instance_labels = np.zeros((voxels.shape[0],), dtype=int)\n",
    "    instance_labels[semantic_mask] = eval_result['instance_labels'].F.cpu().numpy()\n",
    "\n",
    "    instance_output_remap = np.zeros((voxels.shape[0],), dtype=int)\n",
    "    instance_output_remap[semantic_mask] = torch.argmax(instance_output_remap_logits.cpu(), dim=1).numpy()\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    palette = rng.random((len(np.unique(instance_labels)), 3))\n",
    "\n",
    "    id2color = {uid: palette[i] for i, uid in enumerate(np.unique(instance_labels))}\n",
    "\n",
    "    for idx in np.unique(batch_idx):\n",
    "        mask = batch_idx == idx\n",
    "        cloud_voxels = voxels[mask]\n",
    "\n",
    "        cloud_semantic_output = semantic_output[mask]\n",
    "        cloud_semantic_labels = semantic_labels[mask]\n",
    "\n",
    "        cloud_centroid_score_output = centroid_score_output[mask]\n",
    "        cloud_centroid_score_labels = centroid_score_labels[mask]\n",
    "\n",
    "        cloud_offset_output = offset_output[mask]\n",
    "        cloud_offset_labels = offset_labels[mask]\n",
    "\n",
    "        cloud_instance_output = instance_output[mask]\n",
    "        cloud_instance_output_remap = instance_output_remap[mask]\n",
    "        cloud_instance_labels = instance_labels[mask]\n",
    "\n",
    "        mask = centroid_batch_idx == idx\n",
    "        cloud_centroid_voxels = centroid_voxels[mask]\n",
    "        cloud_centroid_confidence_output = centroid_confidence_output[mask]\n",
    "\n",
    "        pcd_gt.points = o3d.utility.Vector3dVector(cloud_voxels)\n",
    "        pcd_gt.translate((20 / VOXEL_SIZE, 0, 0))\n",
    "        pcd_pred.points = o3d.utility.Vector3dVector(cloud_voxels)\n",
    "\n",
    "        colors = trainer.dataset.class_colormap[cloud_semantic_labels] / 255.0\n",
    "        pcd_gt.colors = o3d.utility.Vector3dVector(colors)\n",
    "        colors = trainer.dataset.class_colormap[cloud_semantic_output] / 255.0\n",
    "        pcd_pred.colors = o3d.utility.Vector3dVector(colors)\n",
    "        o3d.visualization.draw_geometries([pcd_pred, pcd_gt])\n",
    "\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        cmap_spheres = plt.get_cmap('inferno')\n",
    "        colors = cmap(cloud_centroid_score_labels[:, 0])[:, :3]\n",
    "        pcd_gt.colors = o3d.utility.Vector3dVector(colors)\n",
    "        colors = cmap(cloud_centroid_score_output[:, 0])[:, :3]\n",
    "        pcd_pred.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "        spheres = []\n",
    "        for i in np.unique(cloud_instance_output):\n",
    "            center = centroid_voxels[i]\n",
    "            confidence = centroid_confidence_output[i][0]\n",
    "\n",
    "            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=1.5)\n",
    "            sphere.translate(center)\n",
    "            color = cmap_spheres(confidence)[:3]\n",
    "            sphere.paint_uniform_color(color)\n",
    "            spheres.append(sphere)\n",
    "\n",
    "        o3d.visualization.draw_geometries([pcd_pred, pcd_gt] + spheres)\n",
    "\n",
    "        voxels_disp_output = cloud_voxels + cloud_offset_output\n",
    "        voxels_disp_labels = cloud_voxels + cloud_offset_labels\n",
    "\n",
    "        pcd_gt.points = o3d.utility.Vector3dVector(voxels_disp_labels)\n",
    "        pcd_gt.translate((20 / VOXEL_SIZE, 0, 0))\n",
    "        pcd_pred.points = o3d.utility.Vector3dVector(voxels_disp_output)\n",
    "\n",
    "        colors = trainer.dataset.class_colormap[cloud_semantic_labels] / 255.0\n",
    "        pcd_gt.colors = o3d.utility.Vector3dVector(colors)\n",
    "        colors = trainer.dataset.class_colormap[cloud_semantic_output] / 255.0\n",
    "        pcd_pred.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "        o3d.visualization.draw_geometries([pcd_pred, pcd_gt] + spheres)\n",
    "\n",
    "        pcd_gt.points = o3d.utility.Vector3dVector(cloud_voxels)\n",
    "        pcd_gt.translate((20 / VOXEL_SIZE, 0, 0))\n",
    "        pcd_pred.points = o3d.utility.Vector3dVector(cloud_voxels)\n",
    "\n",
    "        colors = np.array([id2color[i] for i in cloud_instance_labels], dtype=np.float64)\n",
    "        pcd_gt.colors = o3d.utility.Vector3dVector(colors)\n",
    "        colors = np.array([id2color[i] for i in cloud_instance_output_remap], dtype=np.float64)\n",
    "        pcd_pred.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "        o3d.visualization.draw_geometries([pcd_pred, pcd_gt] + spheres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
