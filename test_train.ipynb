{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"XDG_SESSION_TYPE\"] = \"x11\"\n",
    "os.environ.pop(\"WAYLAND_DISPLAY\", None)\n",
    "\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from EHydro_TreeUnet.trainers import TreeProjectorTrainer\n",
    "from torchsparse.nn import functional as F\n",
    "\n",
    "F.set_kmap_mode(\"hashmap_on_the_fly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "TREE_PROJECTOR_DIR = Path(os.environ.get('TREE_PROJECTOR_DIR', Path.home() / 'tree_projector'))\n",
    "DATASET_FOLDER = 'MixedDataset'\n",
    "VERSION_NAME = 'tree_projector_VS-0.2_DA-48_E-3_V2'\n",
    "\n",
    "VOXEL_SIZE = 0.3\n",
    "FEAT_KEYS = ['intensity']\n",
    "CENTROID_SIGMA = 1.5\n",
    "TRAIN_PCT = 0.9\n",
    "DATA_AUGMENTATION_COEF = 48\n",
    "YAW_RANGE = (0.0, 360.0)\n",
    "TILT_RANGE = (-5.0, 5.0)\n",
    "SCALE_RANGE = (0.9, 1.2)\n",
    "\n",
    "TRAINING = True\n",
    "EPOCHS = 3\n",
    "START_ON_EPOCH = 0\n",
    "BATCH_SIZE = 1\n",
    "SEMANTIC_LOSS_COEF = 1.0\n",
    "CENTROID_LOSS_COEF = 1.0\n",
    "OFFSET_LOSS_COEF = 1.0\n",
    "INSTANCE_LOSS_COEF = 1.0\n",
    "\n",
    "RESNET_BLOCKS = [\n",
    "    (3, 16, 3, 1),\n",
    "    (3, 32, 3, 2),\n",
    "    (3, 64, 3, 2),\n",
    "    (3, 128, 3, 2),\n",
    "    (1, 128, (1, 1, 3), (1, 1, 2)),\n",
    "]\n",
    "LATENT_DIM = 512\n",
    "INSTANCE_DENSITY = 0.01\n",
    "SCORE_THRES = 0.1\n",
    "CENTROID_THRES = 0.2\n",
    "DESCRIPTOR_DIM = 64\n",
    "\n",
    "CHARTS_IGNORE_CLASS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(arr: np.ndarray, window: int) -> np.ndarray:\n",
    "    if window <= 1:\n",
    "        return arr\n",
    "\n",
    "    kernel = np.ones(window, dtype=float)\n",
    "    if arr.ndim == 1:\n",
    "        denom = np.convolve(np.ones_like(arr), kernel, mode=\"same\")\n",
    "        return np.convolve(arr, kernel, mode=\"same\") / denom\n",
    "\n",
    "    # 2-D: suavizar cada columna por separado\n",
    "    smoothed = np.empty_like(arr, dtype=float)\n",
    "    denom = np.convolve(np.ones(arr.shape[0]), kernel, mode=\"same\")\n",
    "    for c in range(arr.shape[1]):\n",
    "        smoothed[:, c] = np.convolve(arr[:, c], kernel, mode=\"same\") / denom\n",
    "    return smoothed\n",
    "        \n",
    "def gen_charts(trainer, losses, stats, training: bool, window: int = 1, ignore_class = []):\n",
    "    keys = stats[0].keys()\n",
    "    stats = {k: np.array([d[k] for d in stats]) for k in keys}\n",
    "\n",
    "    loss = np.asarray(losses)\n",
    "    loss_s = np.clip(smooth(loss[:, 0], window), 0.0, 10.0)\n",
    "    loss_sem_s = np.clip(smooth(loss[:, 1], window), 0.0, 10.0)\n",
    "    loss_centroid_s = np.clip(smooth(loss[:, 2], window), 0.0, 10.0)\n",
    "    loss_inst_s = np.clip(smooth(loss[:, 3], window), 0.0, 10.0)\n",
    "\n",
    "    iou_semantic = smooth(stats['iou_semantic'], window)\n",
    "    mean_iou_semantic = smooth(stats['mean_iou_semantic'], window)\n",
    "    precision_semantic = smooth(stats['precision_semantic'], window)\n",
    "    mean_precision_semantic = smooth(stats['mean_precision_semantic'], window)\n",
    "    recall_semantic = smooth(stats['recall_semantic'], window)\n",
    "    mean_recall_semantic = smooth(stats['mean_recall_semantic'], window)\n",
    "    f1_semantic = smooth(stats['f1_semantic'], window)\n",
    "    mean_f1_semantic = smooth(stats['mean_f1_semantic'], window)\n",
    "\n",
    "    mean_iou_instance = smooth(stats['mean_iou_instance'], window)\n",
    "\n",
    "    # --- Global loss -----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Semantic loss ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_sem_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Semantic loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Centroid loss ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_centroid_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Centroid loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Instance loss ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_inst_s, label=f\"{'Training' if training else 'Inference'} Loss (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Instance loss evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Semantic IoU -----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for c in range(trainer.dataset.num_classes):\n",
    "        if trainer.dataset.class_names[c] in ignore_class:\n",
    "            continue\n",
    "        \n",
    "        plt.plot(iou_semantic[:, c], label=trainer.dataset.class_names[c])\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"IoU\")\n",
    "    plt.title(f\"Semantic IoU evolution during {'Training' if training else 'Inference'} (MA{window})\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Semantic mIoU ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(mean_iou_semantic, label=f\"{'Training' if training else 'Inference'} mIoU (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"mIoU\")\n",
    "    plt.title(f\"Semantic mIoU evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # --- Instance mIoU ----------------------------------------------------\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(mean_iou_instance, label=f\"{'Training' if training else 'Inference'} Instance mIoU (MA{window})\")\n",
    "    plt.xlabel(\"Step\"); plt.ylabel(\"mIoU\")\n",
    "    plt.title(f\"Instance mIoU evolution during {'Training' if training else 'Inference'}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    column_names = ['IoU', 'Precision', 'Recall', 'F1']\n",
    "    row_names = [trainer.dataset.class_names[c] for c in range(trainer.dataset.num_classes) if trainer.dataset.class_names[c] not in ignore_class]\n",
    "    row_names.append('Mean')\n",
    "\n",
    "    iou_semantic_arr = stats['iou_semantic']\n",
    "    prec_arr = stats['precision_semantic']\n",
    "    recall_arr = stats['recall_semantic']\n",
    "    f1_arr = stats['f1_semantic']\n",
    "\n",
    "    data = [\n",
    "        [iou_semantic_arr[:, c].mean(), prec_arr[:, c].mean(), recall_arr[:, c].mean(), f1_arr[:, c].mean()]\n",
    "    for c in range(trainer.dataset.num_classes) if trainer.dataset.class_names[c] not in ignore_class]\n",
    "\n",
    "    means = np.array(data).mean(axis=0)\n",
    "    data.append(list(means))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names, index=row_names)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "InstanceHead.__init__() got an unexpected keyword argument 'tau'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTreeProjectorTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree_projector_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTREE_PROJECTOR_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATASET_FOLDER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERSION_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvoxel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVOXEL_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeat_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFEAT_KEYS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentroid_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCENTROID_SIGMA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_pct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_PCT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_augmentation_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_AUGMENTATION_COEF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43myaw_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYAW_RANGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtilt_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTILT_RANGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCALE_RANGE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAINING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_on_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTART_ON_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msemantic_loss_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEMANTIC_LOSS_COEF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentroid_loss_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCENTROID_LOSS_COEF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset_loss_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOFFSET_LOSS_COEF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_loss_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINSTANCE_LOSS_COEF\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRESNET_BLOCKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLATENT_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_density\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINSTANCE_DENSITY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentroid_thres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCENTROID_THRES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescriptor_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR_DIM\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAINING:\n\u001b[1;32m     32\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/EHydro_TreeUnet/EHydro_TreeUnet/trainers/projector_trainer.py:132\u001b[0m, in \u001b[0;36mTreeProjectorTrainer.__init__\u001b[0;34m(self, tree_projector_dir, dataset_folder, version_name, voxel_size, feat_keys, centroid_sigma, train_pct, data_augmentation_coef, yaw_range, tilt_range, scale_range, training, epochs, start_on_epoch, batch_size, semantic_loss_coef, centroid_loss_coef, offset_loss_coef, instance_loss_coef, resnet_blocks, latent_dim, instance_density, centroid_thres, descriptor_dim)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mTreeProjector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresnet_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresnet_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_density\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_density\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentroid_thres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentroid_thres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescriptor_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescriptor_dim\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m total_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m    143\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad)\n",
      "File \u001b[0;32m~/EHydro_TreeUnet/EHydro_TreeUnet/models/tree_projector.py:35\u001b[0m, in \u001b[0;36mTreeProjector.__init__\u001b[0;34m(self, in_channels, num_classes, resnet_blocks, latent_dim, instance_density, centroid_thres, descriptor_dim)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentroid_head \u001b[38;5;241m=\u001b[39m CentroidHead(latent_dim)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_head \u001b[38;5;241m=\u001b[39m OffsetHead(latent_dim)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance_head \u001b[38;5;241m=\u001b[39m \u001b[43mInstanceHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptor_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentroid_thres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_density\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_density\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: InstanceHead.__init__() got an unexpected keyword argument 'tau'"
     ]
    }
   ],
   "source": [
    "trainer = TreeProjectorTrainer(\n",
    "    tree_projector_dir=TREE_PROJECTOR_DIR,\n",
    "    dataset_folder=DATASET_FOLDER,\n",
    "    version_name=VERSION_NAME,\n",
    "\n",
    "    voxel_size=VOXEL_SIZE,\n",
    "    feat_keys=FEAT_KEYS,\n",
    "    centroid_sigma=CENTROID_SIGMA,\n",
    "    train_pct=TRAIN_PCT,\n",
    "    data_augmentation_coef=DATA_AUGMENTATION_COEF,\n",
    "    yaw_range=YAW_RANGE,\n",
    "    tilt_range=TILT_RANGE,\n",
    "    scale_range=SCALE_RANGE,\n",
    "\n",
    "    training=TRAINING,\n",
    "    epochs=EPOCHS,\n",
    "    start_on_epoch=START_ON_EPOCH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    semantic_loss_coef=SEMANTIC_LOSS_COEF,\n",
    "    centroid_loss_coef=CENTROID_LOSS_COEF,\n",
    "    offset_loss_coef=OFFSET_LOSS_COEF,\n",
    "    instance_loss_coef=INSTANCE_LOSS_COEF,\n",
    "\n",
    "    resnet_blocks=RESNET_BLOCKS,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    instance_density=INSTANCE_DENSITY,\n",
    "    score_thres=SCORE_THRES,\n",
    "    centroid_thres=CENTROID_THRES,\n",
    "    descriptor_dim=DESCRIPTOR_DIM\n",
    ")\n",
    "\n",
    "if TRAINING:\n",
    "    trainer.train()\n",
    "    stats = trainer.stats\n",
    "    losses = trainer.losses\n",
    "    gen_charts(trainer=trainer, losses=trainer.losses, stats=trainer.stats, training=True, window=1, ignore_class=CHARTS_IGNORE_CLASS)\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd2 = o3d.geometry.PointCloud()\n",
    "for voxels, semantic_output, semantic_labels, centroid_score_output, centroid_score_labels, offset_output, offset_labels, instance_output, instance_labels, centroid_voxels, centroid_confidence_output in trainer.eval():\n",
    "    continue\n",
    "    batch_idx = voxels[:, 0]\n",
    "    centroid_batch_idx = centroid_voxels[:, 0]\n",
    "    voxels = voxels[:, 1:]\n",
    "    centroid_voxels = centroid_voxels[:, 1:]\n",
    "\n",
    "    for idx in np.unique(batch_idx):\n",
    "        mask = batch_idx == idx\n",
    "        cloud_voxels = voxels[mask]\n",
    "        cloud_semantic_output = semantic_output[mask]\n",
    "        cloud_semantic_labels = semantic_labels[mask]\n",
    "        cloud_centroid_score_output = centroid_score_output[mask]\n",
    "        cloud_centroid_score_labels = centroid_score_labels[mask]\n",
    "        cloud_instance_output = instance_output[mask]\n",
    "        cloud_instance_labels = instance_labels[mask]\n",
    "\n",
    "        mask = centroid_batch_idx == idx\n",
    "        cloud_centroid_voxels = centroid_voxels[mask]\n",
    "        cloud_centroid_confidence_output = centroid_confidence_output[mask]\n",
    "\n",
    "        pcd.points = o3d.utility.Vector3dVector(cloud_voxels)\n",
    "\n",
    "        colors = trainer.dataset.class_colormap[cloud_semantic_labels] / 255.0\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "        colors = trainer.dataset.class_colormap[cloud_semantic_output] / 255.0\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "\n",
    "        colors = cmap(cloud_centroid_score_labels[:, 0])[:, :3]\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "        colors = cmap(cloud_centroid_score_output[:, 0])[:, :3]\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "        spheres = []\n",
    "        unique_ids = np.unique(cloud_instance_labels)\n",
    "        unique_ids_fg = unique_ids[unique_ids != 0]\n",
    "        colors = trainer.dataset.class_colormap[cloud_semantic_labels] / 255.0\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        for i in range(cloud_centroid_voxels.shape[0]):\n",
    "            center = cloud_centroid_voxels[i]\n",
    "            confidence = cloud_centroid_confidence_output[i][0]\n",
    "\n",
    "            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=1.5)\n",
    "            sphere.translate(center)\n",
    "            color = cmap(confidence)[:3]\n",
    "            sphere.paint_uniform_color(color)\n",
    "            spheres.append(sphere)\n",
    "\n",
    "        o3d.visualization.draw_geometries([pcd] + spheres)\n",
    "        spheres = []\n",
    "        for i in unique_ids_fg:\n",
    "            center = cloud_centroid_voxels[i - 1]\n",
    "            confidence = cloud_centroid_confidence_output[i - 1][0]\n",
    "\n",
    "            sphere = o3d.geometry.TriangleMesh.create_sphere(radius=1.5)\n",
    "            sphere.translate(center)\n",
    "            color = cmap(confidence)[:3]\n",
    "            sphere.paint_uniform_color(color)\n",
    "            spheres.append(sphere)\n",
    "\n",
    "        o3d.visualization.draw_geometries([pcd] + spheres)\n",
    "\n",
    "        rng = np.random.default_rng(0)\n",
    "        palette = rng.random((len(unique_ids), 3))\n",
    "\n",
    "        id2color = {uid: palette[i] for i, uid in enumerate(unique_ids)}\n",
    "        colors = np.array([id2color[i] for i in cloud_instance_labels], dtype=np.float64)\n",
    "\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "        unique_ids = np.unique(cloud_instance_output)\n",
    "        rng = np.random.default_rng(0)\n",
    "        palette = rng.random((len(unique_ids), 3))\n",
    "\n",
    "        id2color = {uid: palette[i] for i, uid in enumerate(unique_ids)}\n",
    "        colors = np.array([id2color[i] for i in cloud_instance_output], dtype=np.float64)\n",
    "\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "gen_charts(trainer=trainer, losses=trainer.losses, stats=trainer.stats, training=False, window=1, ignore_class=CHARTS_IGNORE_CLASS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
