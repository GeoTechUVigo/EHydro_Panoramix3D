{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchsparse\n",
    "import laspy\n",
    "import pandas as pd\n",
    "\n",
    "from EHydro_TreeUnet.tree_projector import TreeProjector\n",
    "from EHydro_TreeUnet.tree_unet import UNet\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as tF\n",
    "from torch.cuda import amp\n",
    "from torchsparse import SparseTensor\n",
    "from torchsparse.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "from torchsparse import backend as ts_backend\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "CHANNELS = [64, 128, 256]\n",
    "LATENT_DIM = 512\n",
    "MAX_INSTANCES = 64\n",
    "TRAIN_PCT = 0.8\n",
    "VOXEL_SIZE = 0.2\n",
    "DATA_AUGMENTATION_COEF = 2.0\n",
    "SEMANTIC_LOSS_COEF = 1.0\n",
    "INSTANCE_LOSS_COEF = 1.0\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FORinstanceDataset:\n",
    "    def __init__(self, voxel_size: float, data_augmentation: float = 1.0, yaw_range = (0, 360), tilt_range = (-5, 5), scale = (0.9, 1.1)) -> None:\n",
    "        self._rng = np.random.default_rng()\n",
    "        self._folder = Path('./datasets/MixedDataset')\n",
    "        self._extensions = ('.laz', '.las')\n",
    "        self._feat_channels = 1\n",
    "        self._num_classes = 3\n",
    "        self._class_names = ['Terrain', 'Stem', 'Live-branches']\n",
    "        self._class_labels = np.array([1, 4, 5])  # IDs en el dataset original. Se remapean a [0, 1, 2]\n",
    "        self._class_colormap = np.array([\n",
    "            [128, 128, 128],# clase 0 - Terrain - gris\n",
    "            [255, 165, 0],  # clase 1 - Stem - naranja\n",
    "            [0, 128, 0],    # clase 2 - Live-branches - verde oscuro\n",
    "        ], dtype=np.uint8)\n",
    "        \n",
    "        self._files = sorted(\n",
    "            [f for f in self._folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in self._extensions],\n",
    "            key=lambda f: f.name\n",
    "        )\n",
    "\n",
    "        self._voxel_size = voxel_size\n",
    "        self._len = int(len(self._files) * data_augmentation)\n",
    "        \n",
    "        self._yaw_range = yaw_range\n",
    "        self._tilt_range = tilt_range\n",
    "        self._scale = scale\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return [self._preprocess(i) for i in range(*idx.indices(len(self)))]\n",
    "        elif isinstance(idx, int):\n",
    "            if idx < 0:\n",
    "                idx += len(self)\n",
    "            if idx < 0 or idx >= len(self):\n",
    "                raise IndexError(\"Index out of range\")\n",
    "            return self._preprocess(idx)\n",
    "        else:\n",
    "            raise TypeError(\"Index must be a slice or an integer\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    @property\n",
    "    def feat_channels(self):\n",
    "        return self._feat_channels\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self._num_classes\n",
    "\n",
    "    @property\n",
    "    def class_names(self):\n",
    "        return self._class_names\n",
    "    \n",
    "    @property\n",
    "    def class_colormap(self):\n",
    "        return self._class_colormap\n",
    "    \n",
    "    def _load_file(self, path):\n",
    "        ext = path.suffix.lower()\n",
    "\n",
    "        coords = ...\n",
    "        feats = ...\n",
    "        semantic_labels = ...\n",
    "        \n",
    "        if ext in ('.las, .laz'):\n",
    "            file = laspy.read(path)\n",
    "\n",
    "            coords = np.vstack((file.x, file.y, file.z)).transpose()\n",
    "            coords -= np.min(coords, axis=0, keepdims=True)\n",
    "            # feats = np.hstack((np.array(file.intensity)[:, None], coords))\n",
    "            feats = np.array(file.intensity)[:, None] / 65535\n",
    "            semantic_labels = np.array(file.classification)\n",
    "            instance_labels = np.array(file.treeID)\n",
    "        else:\n",
    "            raise ValueError(f'Unsopported file extension: {ext}!')\n",
    "\n",
    "        return coords, feats, semantic_labels, instance_labels\n",
    "    \n",
    "    def _agument_data(self, coords):\n",
    "        yaw = np.deg2rad(self._rng.uniform(*self._yaw_range))\n",
    "        pitch = np.deg2rad(self._rng.uniform(*self._tilt_range))\n",
    "        roll = np.deg2rad(self._rng.uniform(*self._tilt_range))\n",
    "        scale = self._rng.uniform(*self._scale)\n",
    "\n",
    "        cy, sy = np.cos(yaw), np.sin(yaw)\n",
    "        cp, sp = np.cos(pitch), np.sin(pitch)\n",
    "        cr, sr = np.cos(roll), np.sin(roll)\n",
    "\n",
    "        rotation_mtx = np.array([[cy*cp,  cy*sp*sr - sy*cr,  cy*sp*cr + sy*sr],\n",
    "                                 [sy*cp,  sy*sp*sr + cy*cr,  sy*sp*cr - cy*sr],\n",
    "                                 [ -sp ,            cp*sr ,            cp*cr ]],\n",
    "                                dtype=coords.dtype)\n",
    "\n",
    "        return (coords @ rotation_mtx.T) * scale\n",
    "        \n",
    "    def _preprocess(self, idx: int):\n",
    "        coords, feat, semantic_labels, instance_labels = self._load_file(self._files[idx % len(self._files)])\n",
    "        if idx >= len(self._files):\n",
    "            coords = self._agument_data(coords)\n",
    "\n",
    "        voxels, indices, inverse_map = sparse_quantize(coords, self._voxel_size, return_index=True, return_inverse=True)\n",
    "        feat = feat[indices]\n",
    "        semantic_labels = semantic_labels[indices]\n",
    "        instance_labels = instance_labels[indices]\n",
    "\n",
    "        voxels = torch.tensor(voxels, dtype=torch.int)\n",
    "        feat = torch.tensor(feat.astype(np.float32), dtype=torch.float)\n",
    "        semantic_labels = torch.tensor(semantic_labels, dtype=torch.long)\n",
    "        instance_labels = torch.tensor(instance_labels, dtype=torch.long)\n",
    "\n",
    "        inputs = SparseTensor(coords=voxels, feats=feat)\n",
    "        semantic_labels = SparseTensor(coords=voxels, feats=semantic_labels)\n",
    "        instance_labels = SparseTensor(coords=voxels, feats=instance_labels)\n",
    "\n",
    "        return {\"inputs\": inputs, \"semantic_labels\": semantic_labels, \"instance_labels\": instance_labels, \"coords\": coords, \"inverse_map\": inverse_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeProjectorTrainer:\n",
    "    def __init__(self):\n",
    "        conv_config = F.conv_config.get_default_conv_config(conv_mode=F.get_conv_mode())\n",
    "        conv_config.kmap_mode = 'hashmap'\n",
    "        F.conv_config.set_global_conv_config(conv_config)\n",
    "\n",
    "        self._dataset = FORinstanceDataset(voxel_size=VOXEL_SIZE, data_augmentation=DATA_AUGMENTATION_COEF)\n",
    "        train_size = int(TRAIN_PCT * len(self._dataset))\n",
    "        val_size = len(self._dataset) - train_size\n",
    "\n",
    "        self._model = TreeProjector(self._dataset.feat_channels, self._dataset.num_classes, MAX_INSTANCES, channels = CHANNELS, latent_dim = LATENT_DIM)\n",
    "        # self._model = UNet(self._dataset.feat_channels, self._dataset.num_classes, base_channels=64, depth=3)\n",
    "        self._device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self._model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self._model.parameters() if p.requires_grad)\n",
    "\n",
    "        print(f\"Parámetros totales: {total_params:,}\")\n",
    "        print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
    "        \n",
    "        train_dataset, val_dataset = random_split(self._dataset, [train_size, val_size])\n",
    "\n",
    "        self._train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=sparse_collate_fn)\n",
    "        self._val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=sparse_collate_fn)\n",
    "\n",
    "        self._criterion_semantic = nn.CrossEntropyLoss()\n",
    "        self._criterion_instance = nn.CrossEntropyLoss()\n",
    "\n",
    "        if not TRAINING:\n",
    "            self._load_weights()\n",
    "\n",
    "        self._model.to(self._device)\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self._dataset\n",
    "\n",
    "    def _load_weights(self):\n",
    "        self._model.load_state_dict(torch.load('./weights/tree_unet_weights.pth'))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _apply_hungarian(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        N, K = logits.shape\n",
    "        device = logits.device\n",
    "\n",
    "        log_p = tF.log_softmax(logits, dim=-1)\n",
    "\n",
    "        uniq = torch.unique(labels)\n",
    "        M    = len(uniq)\n",
    "\n",
    "        cost = torch.empty((M, K), device=device)\n",
    "        for m, g in enumerate(uniq):\n",
    "            mask = (labels == g)\n",
    "            cost[m] = -(log_p[mask].mean(0))\n",
    "\n",
    "        row, col = linear_sum_assignment(cost.detach().cpu())\n",
    "\n",
    "        remapped = torch.full_like(labels, fill_value=-1)\n",
    "        for r, c in zip(row, col):\n",
    "            g = uniq[r]\n",
    "            remapped[labels == g] = c\n",
    "\n",
    "        print(f'Originales: {torch.unique(labels)}\\nRemapeadas: {torch.unique(remapped)}')\n",
    "        return remapped\n",
    "\n",
    "    '''\n",
    "    @torch.no_grad()\n",
    "    def _apply_hungarian(conf: torch.Tensor, gt_labels: torch.Tensor, ignore_val: int = -1) -> torch.Tensor:\n",
    "        M, K = conf.shape\n",
    "        row, col = linear_sum_assignment((-conf).cpu().numpy())\n",
    "\n",
    "        remapped = torch.full_like(gt_labels, fill_value=ignore_val)\n",
    "        for r, c in zip(row, col):\n",
    "            remapped[gt_labels == r] = c\n",
    "\n",
    "        return remapped\n",
    "    '''\n",
    "\n",
    "    def _compute_loss(self, semantic_output, semantic_labels, instance_output = 0, instance_labels = 0):\n",
    "        loss_sem = self._criterion_semantic(semantic_output, semantic_labels)\n",
    "        loss_inst = self._criterion_instance(instance_output, self._apply_hungarian(instance_output, instance_labels))\n",
    "        #loss_inst = 0\n",
    "\n",
    "        return SEMANTIC_LOSS_COEF * loss_sem + INSTANCE_LOSS_COEF * loss_inst\n",
    "    \n",
    "    @torch.no_grad()    \n",
    "    def _compute_metrics(self, pred_labels, gt_labels, num_classes, ignore_index = None):\n",
    "        if ignore_index is not None:\n",
    "            mask = gt_labels != ignore_index\n",
    "            pred_labels, gt_labels = pred_labels[mask], gt_labels[mask]\n",
    "\n",
    "        pred_labels = torch.argmax(pred_labels, dim=1)\n",
    "\n",
    "        C = num_classes\n",
    "        conf = torch.zeros((C, C), dtype=torch.long, device=pred_labels.device)\n",
    "        idx = C * gt_labels + pred_labels\n",
    "        conf += torch.bincount(idx, minlength=C**2).reshape(C, C)\n",
    "\n",
    "        TP = conf.diag()\n",
    "        FP = conf.sum(0) - TP\n",
    "        FN = conf.sum(1) - TP\n",
    "\n",
    "        precision = TP.float() / (TP + FP).clamp(min=1)\n",
    "        recall    = TP.float() / (TP + FN).clamp(min=1)\n",
    "        f1        = 2 * precision * recall / (precision + recall).clamp(min=1e-6)\n",
    "\n",
    "        iou = TP.float() / (TP + FP + FN).clamp(min=1)\n",
    "        miou = iou.mean()\n",
    "\n",
    "        macroP, macroR, macroF = precision.mean(), recall.mean(), f1.mean()\n",
    "\n",
    "        microTP = TP.sum()\n",
    "        microP = microTP.float() / (microTP + FP.sum()).clamp(min=1)\n",
    "        microR = microTP.float() / (microTP + FN.sum()).clamp(min=1)\n",
    "        microF = 2 * microP * microR / (microP + microR).clamp(min=1e-6)\n",
    "\n",
    "        return {\n",
    "            \"confusion\":             conf.cpu().numpy(),\n",
    "            \"iou_per_class\":         iou.cpu().numpy(),\n",
    "            \"miou\":                  miou.cpu().numpy(),\n",
    "            \"precision_per_class\":   precision.cpu().numpy(),\n",
    "            \"recall_per_class\":      recall.cpu().numpy(),\n",
    "            \"f1_per_class\":          f1.cpu().numpy(),\n",
    "            \"precision_macro\":       macroP.cpu().numpy(),\n",
    "            \"recall_macro\":          macroR.cpu().numpy(),\n",
    "            \"f1_macro\":              macroF.cpu().numpy(),\n",
    "            \"precision_micro\":       microP.cpu().numpy(),\n",
    "            \"recall_micro\":          microR.cpu().numpy(),\n",
    "            \"f1_micro\":              microF.cpu().numpy(),\n",
    "        }\n",
    "    \n",
    "    '''\n",
    "    def _compute_iou(self, semantic_output, semantic_labels):\n",
    "        if semantic_output.C.shape != semantic_labels.C.shape or not torch.all(semantic_output.C == semantic_labels.C):\n",
    "            raise ValueError(\"Dimensions doesn't match between semantic labels and output.\")\n",
    "\n",
    "        semantic_output = semantic_output.F.argmax(dim=1)\n",
    "        semantic_labels = semantic_labels.F.view(-1).long()\n",
    "        iou_list = torch.full((self._dataset.num_classes,), float('nan'), device=semantic_output.device)\n",
    "\n",
    "        for cls in range(self._dataset.num_classes):\n",
    "            label_mask = semantic_labels == cls\n",
    "            out_mask = semantic_output == cls\n",
    "\n",
    "            union = (out_mask | label_mask).sum()\n",
    "            if union == 0:\n",
    "                continue\n",
    "\n",
    "            inter = (out_mask & label_mask).sum()\n",
    "            iou_list[cls] = inter.float() / union.float()\n",
    "\n",
    "        valid = ~torch.isnan(iou_list)\n",
    "        miou  = iou_list[valid].mean().item() if valid.any() else float(\"nan\")\n",
    "        return iou_list, miou\n",
    "    '''\n",
    "            \n",
    "    def _gen_charts(self, losses, stats, training):\n",
    "        keys = stats[0].keys()\n",
    "        stats = {k: np.array([d[k] for d in stats]) for k in keys}\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses, label=f\"{'Training' if training else 'Inference'} Loss\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Loss evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['miou'], label=f\"{'Training' if training else 'Inference'} mIoU\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"mIoU\")\n",
    "        plt.title(f\"mIoU evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        iou_arr = np.asarray(stats['iou_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(iou_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"IoU\")\n",
    "        plt.title(f\"IoU evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['precision_macro'], label=f\"{'Training' if training else 'Inference'} precision\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Precision evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        prec_arr = np.asarray(stats['precision_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(prec_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Precision evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['recall_macro'], label=f\"{'Training' if training else 'Inference'} recall\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Recall\")\n",
    "        plt.title(f\"Recall evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        recall_arr = np.asarray(stats['recall_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(recall_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Recall\")\n",
    "        plt.title(f\"Recall evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['f1_macro'], label=f\"{'Training' if training else 'Inference'} F1\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"F1\")\n",
    "        plt.title(f\"F1 evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        f1_arr = np.asarray(stats['f1_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(f1_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"F1\")\n",
    "        plt.title(f\"F1 evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        column_names = ['IoU', 'Precision', 'Recall', 'F1']\n",
    "        row_names = [self._dataset.class_names[c] for c in range(self._dataset.num_classes)]\n",
    "        row_names.append('Mean')\n",
    "\n",
    "        data = [\n",
    "            [iou_arr[c].mean(), prec_arr[c].mean(), recall_arr[c].mean(), f1_arr[c].mean()]\n",
    "        for c in range(self._dataset.num_classes)]\n",
    "\n",
    "        data.append([stats['miou'].mean(), stats['precision_macro'].mean(), stats['recall_macro'].mean(), stats['f1_macro'].mean()])\n",
    "\n",
    "        df = pd.DataFrame(data, columns=column_names, index=row_names)\n",
    "        display(df)\n",
    "    \n",
    "    def train(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), lr=1e-3)\n",
    "        scaler = amp.GradScaler(enabled=True)\n",
    "        losses = []\n",
    "        stats = []\n",
    "\n",
    "        for k, feed_dict in enumerate(self._train_loader):\n",
    "            inputs = feed_dict[\"inputs\"].to(self._device)\n",
    "            semantic_labels = feed_dict[\"semantic_labels\"].to(self._device)\n",
    "            instance_labels = feed_dict[\"instance_labels\"].to(self._device)\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                semantic_output, instance_output = self._model(inputs)\n",
    "                # semantic_output = self._model(inputs)\n",
    "                # loss = self._compute_loss(semantic_output.F, semantic_labels.F)\n",
    "                loss = self._compute_loss(semantic_output.F, semantic_labels.F, instance_output.F, instance_labels.F)\n",
    "                stat = self._compute_metrics(semantic_output.F, semantic_labels.F, num_classes=self._dataset.num_classes)\n",
    "\n",
    "\n",
    "            stats.append(stat)\n",
    "            losses.append(loss.item())\n",
    "            print(f\"[Train step {k + 1}] loss = {loss.item()}; mIoU = {stat['miou']}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            del inputs, semantic_output, semantic_labels\n",
    "\n",
    "        torch.save(self._model.state_dict(), './weights/tree_unet_weights.pth')\n",
    "        self._gen_charts(losses, stats, True)\n",
    "\n",
    "    def eval(self):\n",
    "        self._model.eval()\n",
    "        losses = []\n",
    "        stats = []\n",
    "\n",
    "        # enable torchsparse 2.0 inference\n",
    "        # enable fused and locality-aware memory access optimization\n",
    "        torchsparse.backends.benchmark = True  # type: ignore\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for k, feed_dict in enumerate(self._val_loader):\n",
    "                semantic_labels_cpu = feed_dict[\"semantic_labels\"].F.numpy()\n",
    "                instance_labels_cpu = feed_dict[\"instance_labels\"].F.numpy()\n",
    "                coords = feed_dict[\"coords\"].numpy()\n",
    "                inverse_map = feed_dict[\"inverse_map\"].numpy()\n",
    "\n",
    "                inputs = feed_dict[\"inputs\"].to(self._device)\n",
    "                semantic_labels = feed_dict[\"semantic_labels\"].to(self._device)\n",
    "                instance_labels = feed_dict[\"instance_labels\"].to(self._device)\n",
    "\n",
    "                with amp.autocast(enabled=True):\n",
    "                    semantic_output, instance_output = self._model(inputs)\n",
    "                    # semantic_output = self._model(inputs)\n",
    "                    # loss = self._compute_loss(semantic_output.F, semantic_labels.F)\n",
    "                    loss = self._compute_loss(semantic_output.F, semantic_labels.F, instance_output.F, instance_labels.F)\n",
    "                    stat = self._compute_metrics(semantic_output.F, semantic_labels.F, num_classes=self._dataset.num_classes)\n",
    "\n",
    "                print(f\"[Inference step {k + 1}] loss = {loss.item()}; mIoU = {stat['miou']}\")\n",
    "                losses.append(loss.item())\n",
    "                stats.append(stat)\n",
    "\n",
    "                voxels = semantic_output.C[:, 1:].cpu().numpy()\n",
    "                semantic_output = torch.argmax(semantic_output.F.cpu(), dim=1).numpy()\n",
    "                instance_output = torch.argmax(instance_output.F.cpu(), dim=1).numpy()\n",
    "                # instance_output = np.zeros(semantic_output.shape)\n",
    "\n",
    "                yield voxels, semantic_output, instance_output, semantic_labels_cpu, instance_labels_cpu, coords, inverse_map\n",
    "\n",
    "        self._gen_charts(losses, stats, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntester = TreeProjectorTrainer()\\n\\ntorch.manual_seed(0)\\n\\nN, K = 12, 3                       # 12 puntos, 3 canales (max_inst)\\nsemantic_lbl = torch.randint(0, 4, (N,))\\n\\n# Instancias GT: bloques de 4 puntos\\ninst_lbl = torch.tensor([0]*4 + [1]*4 + [2]*4)\\n\\n# Logits: cada instancia favorece un canal *distinto*\\ninst_out = torch.randn(N, K) * 0.3\\ninst_out[:4, 1] += 3.0   # inst-0 -> canal-1\\ninst_out[4:8, 2] += 3.0  # inst-1 -> canal-2\\ninst_out[8:, 0]  += 3.0  # inst-2 -> canal-0\\n\\n# Pérdida con labels originales (mala correspondencia)\\nloss_bad = tester._criterion_instance(inst_out, inst_lbl)\\n\\n# Pérdida tras húngaro + remapeo\\nremap = tester._apply_hungarian(inst_out, inst_lbl)\\nprint(remap)\\nloss_good = tester._criterion_instance(inst_out, remap)\\n\\nprint(f\"Loss antes  = {loss_bad.item():.3f}\")\\nprint(f\"Loss después = {loss_good.item():.3f}\")\\n\\nprint(inst_out)\\nprint(inst_lbl)\\nprint(remap)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tester = TreeProjectorTrainer()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N, K = 12, 3                       # 12 puntos, 3 canales (max_inst)\n",
    "semantic_lbl = torch.randint(0, 4, (N,))\n",
    "\n",
    "# Instancias GT: bloques de 4 puntos\n",
    "inst_lbl = torch.tensor([0]*4 + [1]*4 + [2]*4)\n",
    "\n",
    "# Logits: cada instancia favorece un canal *distinto*\n",
    "inst_out = torch.randn(N, K) * 0.3\n",
    "inst_out[:4, 1] += 3.0   # inst-0 -> canal-1\n",
    "inst_out[4:8, 2] += 3.0  # inst-1 -> canal-2\n",
    "inst_out[8:, 0]  += 3.0  # inst-2 -> canal-0\n",
    "\n",
    "# Pérdida con labels originales (mala correspondencia)\n",
    "loss_bad = tester._criterion_instance(inst_out, inst_lbl)\n",
    "\n",
    "# Pérdida tras húngaro + remapeo\n",
    "remap = tester._apply_hungarian(inst_out, inst_lbl)\n",
    "print(remap)\n",
    "loss_good = tester._criterion_instance(inst_out, remap)\n",
    "\n",
    "print(f\"Loss antes  = {loss_bad.item():.3f}\")\n",
    "print(f\"Loss después = {loss_good.item():.3f}\")\n",
    "\n",
    "print(inst_out)\n",
    "print(inst_lbl)\n",
    "print(remap)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros totales: 20,753,856\n",
      "Parámetros entrenables: 20,753,856\n",
      "[Train step 1] loss = 1.0620970726013184; mIoU = 0.1959919035434723\n",
      "[Train step 2] loss = 2.4405453205108643; mIoU = 0.17826908826828003\n",
      "[Train step 3] loss = 2.1604232788085938; mIoU = 0.08186735212802887\n",
      "[Train step 4] loss = 1.8931950330734253; mIoU = 0.10502304881811142\n",
      "[Train step 5] loss = 1.5076003074645996; mIoU = 0.112993985414505\n",
      "[Train step 6] loss = 1.4065375328063965; mIoU = 0.12806841731071472\n",
      "[Train step 7] loss = 1.279304027557373; mIoU = 0.1306391954421997\n",
      "[Train step 8] loss = 1.1677042245864868; mIoU = 0.14592084288597107\n",
      "[Train step 9] loss = 0.7889038920402527; mIoU = 0.3251461386680603\n",
      "[Train step 10] loss = 1.0211827754974365; mIoU = 0.20259994268417358\n",
      "[Train step 11] loss = 1.21672785282135; mIoU = 0.06075023114681244\n",
      "[Train step 12] loss = 1.0809663534164429; mIoU = 0.18075916171073914\n",
      "[Train step 13] loss = 0.9887595176696777; mIoU = 0.18889401853084564\n",
      "[Train step 14] loss = 0.9779450297355652; mIoU = 0.21014416217803955\n",
      "[Train step 15] loss = 0.9317864179611206; mIoU = 0.23668059706687927\n",
      "[Train step 16] loss = 0.7029725313186646; mIoU = 0.47264379262924194\n",
      "[Train step 17] loss = 0.8344569206237793; mIoU = 0.2991459369659424\n",
      "[Train step 18] loss = 0.7393143773078918; mIoU = 0.4179695248603821\n",
      "[Train step 19] loss = 0.7910181879997253; mIoU = 0.3963944613933563\n",
      "[Train step 20] loss = 0.7135259509086609; mIoU = 0.549613356590271\n",
      "[Train step 21] loss = 0.6996578574180603; mIoU = 0.48410362005233765\n",
      "[Train step 22] loss = 0.8035289645195007; mIoU = 0.37066978216171265\n",
      "[Train step 23] loss = 0.624507486820221; mIoU = 0.4591321349143982\n",
      "[Train step 24] loss = 0.5282816290855408; mIoU = 0.5168591737747192\n",
      "[Train step 25] loss = 0.7001630663871765; mIoU = 0.46852749586105347\n",
      "[Train step 26] loss = 0.5528755784034729; mIoU = 0.5718436241149902\n",
      "[Train step 27] loss = 0.4615316092967987; mIoU = 0.5544418096542358\n",
      "[Train step 28] loss = 0.5212007164955139; mIoU = 0.5308281779289246\n",
      "[Train step 29] loss = 0.5017472505569458; mIoU = 0.48710498213768005\n",
      "[Train step 30] loss = 0.41399863362312317; mIoU = 0.5870447158813477\n",
      "[Train step 31] loss = 0.36932307481765747; mIoU = 0.5622859597206116\n",
      "[Train step 32] loss = 0.41345739364624023; mIoU = 0.5241225957870483\n",
      "[Train step 33] loss = 0.45991140604019165; mIoU = 0.4987645149230957\n",
      "[Train step 34] loss = 0.32327866554260254; mIoU = 0.5776052474975586\n",
      "[Train step 35] loss = 0.49897804856300354; mIoU = 0.4811374843120575\n",
      "[Train step 36] loss = 0.4076121151447296; mIoU = 0.5315337181091309\n",
      "[Train step 37] loss = 0.722385048866272; mIoU = 0.4030331075191498\n",
      "[Train step 38] loss = 0.41613873839378357; mIoU = 0.5091378092765808\n",
      "[Train step 39] loss = 0.3674331307411194; mIoU = 0.5682243704795837\n",
      "[Train step 40] loss = 0.4864490032196045; mIoU = 0.47083088755607605\n",
      "[Train step 41] loss = 0.3284912407398224; mIoU = 0.5946402549743652\n",
      "[Train step 42] loss = 0.27519094944000244; mIoU = 0.562836766242981\n",
      "[Train step 43] loss = 0.3052237331867218; mIoU = 0.5837308168411255\n",
      "[Train step 44] loss = 0.48516032099723816; mIoU = 0.5041713714599609\n",
      "[Train step 45] loss = 0.38717910647392273; mIoU = 0.5302079916000366\n"
     ]
    }
   ],
   "source": [
    "!export TS_HASHMAP_MAX_MB=0\n",
    "\n",
    "tester = TreeProjectorTrainer()\n",
    "\n",
    "if TRAINING:\n",
    "    tester.train()\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "for voxels, semantic_output, instance_output, semantic_labels, instance_labels, coords, inverse_map in tester.eval():\n",
    "    continue\n",
    "    coords = coords[0]\n",
    "    inverse_map = inverse_map[0]\n",
    "\n",
    "    colors = tester.dataset.class_colormap[semantic_labels[inverse_map]] / 255.0\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    colors = tester.dataset.class_colormap[semantic_output[inverse_map]] / 255.0\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    unique_ids = np.unique(instance_labels)\n",
    "    rng = np.random.default_rng(0)\n",
    "    palette = rng.random((len(unique_ids), 3))\n",
    "\n",
    "    id2color = {uid: palette[i] for i, uid in enumerate(unique_ids)}\n",
    "    colors = np.array([id2color[i] for i in instance_labels], dtype=np.float64)\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[inverse_map])\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    unique_ids = np.unique(instance_output)\n",
    "    rng = np.random.default_rng(0)\n",
    "    palette = rng.random((len(unique_ids), 3))\n",
    "\n",
    "    id2color = {uid: palette[i] for i, uid in enumerate(unique_ids)}\n",
    "    colors = np.array([id2color[i] for i in instance_output], dtype=np.float64)\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[inverse_map])\n",
    "    o3d.visualization.draw_geometries([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
