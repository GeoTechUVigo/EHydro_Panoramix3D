{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff646079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import torch\n",
    "import laspy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from laspy import LasData\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.cuda import amp\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "from torchsparse import SparseTensor\n",
    "\n",
    "from EHydro_TreeUnet import TreeProjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31ceba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_CHUNKS = True\n",
    "USE_STORED = True\n",
    "SAVE_SEGMENTED = True\n",
    "SAVE_ALL_SEGMENTS = True\n",
    "STEP = 50\n",
    "MIN_POINTS_PER_PC = 10000\n",
    "WEIGHTS_FILE = 'tree_projector_weights_x16_0.2.pth'\n",
    "CHANNELS = [16, 32, 64, 128]\n",
    "LATENT_DIM = 256\n",
    "MAX_INSTANCES = 64\n",
    "VOXEL_SIZE = 0.2\n",
    "\n",
    "class_names = ['Terrain', 'Low Vegetation', 'Stem', 'Canopy']\n",
    "class_colormap = np.array([\n",
    "    [128, 128, 128], # clase 0 - Terrain - gris\n",
    "    [147, 255, 138], # clase 1 - Low vegetation - verde claro\n",
    "    [255, 165, 0],   # clase 2 - Stem - naranja\n",
    "    [0, 128, 0],     # clase 3 - Canopy - verde oscuro\n",
    "], dtype=np.uint8)\n",
    "\n",
    "pointcloud_folder = Path('/mnt/c/Users/samue/Desktop/PointClouds')\n",
    "input_folder = pointcloud_folder / 'input'\n",
    "output_folder = pointcloud_folder / 'output'\n",
    "segmented_folder = pointcloud_folder / 'segmented'\n",
    "segmented_by_folder = pointcloud_folder / 'segmented/by_folders'\n",
    "\n",
    "input_folder.mkdir(parents=True, exist_ok=True)\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "segmented_folder.mkdir(parents=True, exist_ok=True)\n",
    "segmented_by_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a886308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkerize(file: LasData, ext: str):\n",
    "    points = file.points\n",
    "    coords = np.vstack((file.x, file.y)).transpose()\n",
    "\n",
    "    idx  = np.floor_divide(coords, STEP).astype(int)\n",
    "    idx -= idx.min(axis=0)\n",
    "\n",
    "    idx = np.ravel_multi_index(idx.T, idx.max(axis=0) + 1)\n",
    "    chunks = []\n",
    "    for i, unique_idx in enumerate(tqdm(np.unique(idx))):\n",
    "        chunk_points = points[idx == unique_idx]\n",
    "        if len(chunk_points) < MIN_POINTS_PER_PC:\n",
    "            continue\n",
    "\n",
    "        chunk_file = laspy.LasData(file.header)\n",
    "        chunk_file.points = chunk_points\n",
    "        chunks.append(chunk_file)\n",
    "        \n",
    "        if SAVE_CHUNKS:\n",
    "            chunk_file.write(output_folder / f'plot_{i}{ext}')\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9363e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:29<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "extensions = ('.laz', '.las')\n",
    "point_clouds = sorted(\n",
    "            [f for f in input_folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in extensions],\n",
    "            key=lambda f: f.name\n",
    "        )\n",
    "\n",
    "model = TreeProjector(1, len(class_names), MAX_INSTANCES, channels = CHANNELS, latent_dim = LATENT_DIM)\n",
    "model.load_state_dict(torch.load(Path('./weights') / WEIGHTS_FILE))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "for point_cloud in point_clouds:\n",
    "    if USE_STORED:\n",
    "        chunks = [laspy.read(f) for f in output_folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in extensions]\n",
    "    else:\n",
    "        chunks = chunkerize(laspy.read(point_cloud), point_cloud.suffix.lower())\n",
    "\n",
    "    segmented_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(tqdm(chunks)):\n",
    "        coords = np.vstack((chunk.x, chunk.y, chunk.z)).transpose()\n",
    "        min_coords = coords.min(axis=0)\n",
    "        coords -= min_coords\n",
    "\n",
    "        intensity = np.array(chunk.intensity)[:, None]\n",
    "        min_intensity = np.min(intensity)\n",
    "        max_intensity = np.max(intensity)\n",
    "        i_norm = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "\n",
    "        voxels, indices, inverse_map = sparse_quantize(coords, VOXEL_SIZE, return_index=True, return_inverse=True)\n",
    "\n",
    "        voxels = torch.tensor(voxels, dtype=torch.int).to(device)\n",
    "        batch_index = torch.zeros((voxels.shape[0], 1), dtype=torch.int, device=voxels.device)\n",
    "        voxels = torch.cat([batch_index, voxels], dim=1)\n",
    "        feat = torch.tensor(i_norm.astype(np.float32), dtype=torch.float).to(device)\n",
    "\n",
    "        inputs = SparseTensor(coords=voxels, feats=feat)\n",
    "        with amp.autocast(enabled=True):\n",
    "            semantic_output, instance_output = model(inputs)\n",
    "\n",
    "        voxels = semantic_output.C.cpu().numpy()\n",
    "        semantic_output = torch.argmax(semantic_output.F.cpu(), dim=1).numpy()\n",
    "        instance_output = torch.argmax(instance_output.F.cpu(), dim=1).numpy()\n",
    "        instance_output[(semantic_output == 0) | (semantic_output == 1)] = -1\n",
    "\n",
    "        semantic_output = semantic_output[inverse_map]\n",
    "        instance_output = (instance_output + (64 * i))[inverse_map]\n",
    "\n",
    "        out_file = laspy.LasData(header=chunk.header, points=chunk.points.copy())\n",
    "        out_file.add_extra_dims([laspy.ExtraBytesParams(name=\"semantic_pred\", type=np.int16), laspy.ExtraBytesParams(name=\"instance_pred\", type=np.int32)])\n",
    "        out_file.semantic_pred = semantic_output\n",
    "        out_file.instance_pred = instance_output\n",
    "\n",
    "        segmented_chunks.append(out_file)\n",
    "        if SAVE_ALL_SEGMENTS:\n",
    "            out_file.write(segmented_by_folder / f'plot_{i}.las')\n",
    "\n",
    "    with laspy.open(segmented_folder / 'combined.las', mode='w', header=chunks[0].header) as file:\n",
    "        for chunk in segmented_chunks:\n",
    "            file.write_points(chunk.points)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
