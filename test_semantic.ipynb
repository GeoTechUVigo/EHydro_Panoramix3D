{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchsparse\n",
    "import laspy\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from plyfile import PlyData\n",
    "\n",
    "from EHydro_TreeUnet.tree_projector import TreeProjector\n",
    "from EHydro_TreeUnet.tree_unet import UNet\n",
    "from pathlib import Path\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as tF\n",
    "from torch.cuda import amp\n",
    "from torchsparse import SparseTensor\n",
    "from torchsparse.nn import functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "\n",
    "CHANNELS = [16, 32, 64, 128]\n",
    "LATENT_DIM = 512\n",
    "MAX_INSTANCES = 64\n",
    "TRAIN_PCT = 0.8\n",
    "VOXEL_SIZE = 0.1\n",
    "DATA_AUGMENTATION_COEF = 2.0\n",
    "SEMANTIC_LOSS_COEF = 1.0\n",
    "INSTANCE_LOSS_COEF = 0.0\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, files, voxel_size: float, data_augmentation: float = 1.0, yaw_range = (0, 360), tilt_range = (-5, 5), scale = (0.9, 1.1)) -> None:\n",
    "        self._rng = np.random.default_rng()\n",
    "        self._files = files\n",
    "\n",
    "        self._voxel_size = voxel_size\n",
    "        self._len = int(len(self._files) * data_augmentation)\n",
    "        \n",
    "        self._yaw_range = yaw_range\n",
    "        self._tilt_range = tilt_range\n",
    "        self._scale = scale\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return [self._preprocess(i) for i in range(*idx.indices(len(self)))]\n",
    "        elif isinstance(idx, int):\n",
    "            if idx < 0:\n",
    "                idx += len(self)\n",
    "            if idx < 0 or idx >= len(self):\n",
    "                raise IndexError(\"Index out of range\")\n",
    "            return self._preprocess(idx)\n",
    "        else:\n",
    "            raise TypeError(\"Index must be a slice or an integer\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def _load_file(self, path):\n",
    "        ext = path.suffix.lower()\n",
    "\n",
    "        coords = ...\n",
    "        feats = ...\n",
    "        semantic_labels = ...\n",
    "        \n",
    "        if ext in ('.las, .laz'):\n",
    "            file = laspy.read(path)\n",
    "\n",
    "            z = np.asarray(file.z)\n",
    "            coords = np.vstack((file.x, file.y, z)).transpose()\n",
    "            # feats = np.hstack((np.array(file.intensity)[:, None], coords))\n",
    "            min_z = np.min(z)\n",
    "            feats = np.array(file.intensity) / 65535\n",
    "            feats = np.column_stack((feats, (z - min_z) / (np.max(z) - min_z)))\n",
    "            semantic_labels = np.array(file.classification)\n",
    "            instance_labels = np.array(file.treeID)\n",
    "        else:\n",
    "            raise ValueError(f'Unsopported file extension: {ext}!')\n",
    "\n",
    "        return coords, feats, semantic_labels, instance_labels\n",
    "    \n",
    "    def _agument_data(self, coords):\n",
    "        yaw = np.deg2rad(self._rng.uniform(*self._yaw_range))\n",
    "        pitch = np.deg2rad(self._rng.uniform(*self._tilt_range))\n",
    "        roll = np.deg2rad(self._rng.uniform(*self._tilt_range))\n",
    "        scale = self._rng.uniform(*self._scale)\n",
    "\n",
    "        cy, sy = np.cos(yaw), np.sin(yaw)\n",
    "        cp, sp = np.cos(pitch), np.sin(pitch)\n",
    "        cr, sr = np.cos(roll), np.sin(roll)\n",
    "\n",
    "        rotation_mtx = np.array([[cy*cp,  cy*sp*sr - sy*cr,  cy*sp*cr + sy*sr],\n",
    "                                 [sy*cp,  sy*sp*sr + cy*cr,  sy*sp*cr - cy*sr],\n",
    "                                 [ -sp ,            cp*sr ,            cp*cr ]],\n",
    "                                dtype=coords.dtype)\n",
    "\n",
    "        return (coords @ rotation_mtx.T) * scale\n",
    "        \n",
    "    def _preprocess(self, idx: int):\n",
    "        coords, feat, semantic_labels, instance_labels = self._load_file(self._files[idx % len(self._files)])\n",
    "        if idx >= len(self._files):\n",
    "            coords = self._agument_data(coords)\n",
    "\n",
    "        coords -= np.min(coords, axis=0, keepdims=True)\n",
    "\n",
    "        voxels, indices, inverse_map = sparse_quantize(coords, self._voxel_size, return_index=True, return_inverse=True)\n",
    "        feat = feat[indices]\n",
    "        semantic_labels = semantic_labels[indices]\n",
    "        instance_labels = instance_labels[indices]\n",
    "\n",
    "        voxels = torch.tensor(voxels, dtype=torch.int)\n",
    "        feat = torch.tensor(feat.astype(np.float32), dtype=torch.float)\n",
    "        semantic_labels = torch.tensor(semantic_labels, dtype=torch.long)\n",
    "        instance_labels = torch.tensor(instance_labels, dtype=torch.long)\n",
    "\n",
    "        inputs = SparseTensor(coords=voxels, feats=feat)\n",
    "        semantic_labels = SparseTensor(coords=voxels, feats=semantic_labels)\n",
    "        instance_labels = SparseTensor(coords=voxels, feats=instance_labels)\n",
    "\n",
    "        return {\"inputs\": inputs, \"semantic_labels\": semantic_labels, \"instance_labels\": instance_labels, \"coords\": coords, \"inverse_map\": inverse_map}\n",
    "\n",
    "\n",
    "class MixedDataset:\n",
    "    def __init__(self, voxel_size: float, data_augmentation: float = 1.0, yaw_range = (0, 360), tilt_range = (-5, 5), scale = (0.9, 1.1)) -> None:\n",
    "        self._folder = Path('./datasets/MixedDataset')\n",
    "        self._extensions = ('.laz', '.las')\n",
    "\n",
    "        self._feat_channels = 2\n",
    "        self._num_classes = 4\n",
    "        self._class_names = ['Terrain', 'Low Vegetation', 'Stem', 'Canopy']\n",
    "        self._class_colormap = np.array([\n",
    "            [128, 128, 128], # clase 0 - Terrain - gris\n",
    "            [147, 255, 138], # clase 1 - Low vegetation - verde claro\n",
    "            [255, 165, 0],   # clase 2 - Stem - naranja\n",
    "            [0, 128, 0],     # clase 3 - Canopy - verde oscuro\n",
    "        ], dtype=np.uint8)\n",
    "\n",
    "        files = sorted(\n",
    "            [f for f in self._folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in self._extensions],\n",
    "            key=lambda f: f.name\n",
    "        )\n",
    "\n",
    "        train_idx = int(TRAIN_PCT * len(files))\n",
    "        self._train_dataset = Dataset(files[:train_idx], voxel_size, data_augmentation, yaw_range, tilt_range, scale)\n",
    "        self._val_dataset = Dataset(files[train_idx:], voxel_size)\n",
    "\n",
    "    @property\n",
    "    def feat_channels(self):\n",
    "        return self._feat_channels\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self._num_classes\n",
    "\n",
    "    @property\n",
    "    def class_names(self):\n",
    "        return self._class_names\n",
    "    \n",
    "    @property\n",
    "    def class_colormap(self):\n",
    "        return self._class_colormap\n",
    "    \n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self._train_dataset\n",
    "    \n",
    "    @property\n",
    "    def val_dataset(self):\n",
    "        return self._val_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScanNet:\n",
    "    def __init__(self, voxel_size: float, data_augmentation: float = 1.0, yaw_range = (0, 360), tilt_range = (-5, 5), scale = (0.9, 1.1)) -> None:\n",
    "        self._rng = np.random.default_rng()\n",
    "        self._folder = Path('./datasets/ScanNet/')\n",
    "        self._extensions = ('.ply')\n",
    "        self._feat_channels = 3\n",
    "        self._num_classes = 41\n",
    "        self._class_names = [\n",
    "            \"wall\", \"floor\", \"cabinet\", \"bed\", \"chair\", \"sofa\", \"table\", \"door\", \"window\", \"bookshelf\",\n",
    "            \"picture\", \"counter\", \"desk\", \"curtain\", \"refrigerator\", \"shower curtain\", \"toilet\", \"sink\",\n",
    "            \"bathtub\", \"otherfurniture\"\n",
    "        ]\n",
    "        self._class_labels = np.array([1, 4, 5])\n",
    "        cmap = plt.get_cmap('tab10', self._num_classes)\n",
    "        colors = cmap(np.arange(self._num_classes))[:, :3]\n",
    "        self._class_colormap = (colors * 255).astype(np.uint8)\n",
    "        \n",
    "        self._files = sorted(\n",
    "            [f for f in self._folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in self._extensions and f.stem.endswith('.labels')],\n",
    "            key=lambda f: f.name\n",
    "        )\n",
    "\n",
    "        self._voxel_size = voxel_size\n",
    "        self._len = int(len(self._files) * data_augmentation)\n",
    "        \n",
    "        self._yaw_range = yaw_range\n",
    "        self._tilt_range = tilt_range\n",
    "        self._scale = scale\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return [self._preprocess(self.files[i]) for i in range(*idx.indices(len(self)))]\n",
    "        elif isinstance(idx, int):\n",
    "            if idx < 0:\n",
    "                idx += len(self)\n",
    "            if idx < 0 or idx >= len(self):\n",
    "                raise IndexError(\"Index out of range\")\n",
    "            return self._preprocess(idx)\n",
    "        else:\n",
    "            raise TypeError(\"Index must be a slice or an integer\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    @property\n",
    "    def feat_channels(self):\n",
    "        return self._feat_channels\n",
    "    \n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return self._num_classes\n",
    "\n",
    "    @property\n",
    "    def class_names(self):\n",
    "        return self._class_names\n",
    "    \n",
    "    @property\n",
    "    def class_colormap(self):\n",
    "        return self._class_colormap\n",
    "    \n",
    "    def _load_file(self, path):\n",
    "        ext = path.suffix.lower()\n",
    "\n",
    "        coords = ...\n",
    "        feats = ...\n",
    "        semantic_labels = ...\n",
    "        \n",
    "        if ext in ('.las, .laz'):\n",
    "            file = laspy.read(path)\n",
    "\n",
    "            coords = np.vstack((file.x, file.y, file.z)).transpose()\n",
    "            coords -= np.min(coords, axis=0, keepdims=True)\n",
    "            # feats = np.hstack((np.array(file.intensity)[:, None], coords))\n",
    "            I = np.array(file.intensity)\n",
    "            p1, p99 = np.percentile(I, [1, 99])\n",
    "            I_norm = np.clip((I - p1) / (p99 - p1), 0, 1)\n",
    "            I_norm = I_norm - np.median(I_norm)\n",
    "            feats = I_norm[:, None]\n",
    "            \n",
    "            semantic_labels = np.array(file.classification)\n",
    "            instance_labels = np.array(file.treeID)\n",
    "        elif ext == '.ply':\n",
    "            ply = PlyData.read(str(path))\n",
    "            v   = ply['vertex']\n",
    "\n",
    "            coords = np.stack([v['x'], v['y'], v['z']], axis=-1).astype(np.float32)\n",
    "            coords -= np.min(coords, axis=0, keepdims=True)\n",
    "            feats = np.stack([v['red'], v['green'], v['blue']], axis=-1).astype(np.float32) / 256\n",
    "\n",
    "            semantic_labels = v['label'].astype(np.int64)\n",
    "            instance_labels = np.zeros(semantic_labels.shape)\n",
    "        else:\n",
    "            raise ValueError(f'Unsopported file extension: {ext}!')\n",
    "\n",
    "        return coords, feats, semantic_labels, instance_labels\n",
    "    \n",
    "    def _agument_data(self, coords):\n",
    "        yaw = np.deg2rad(self._rng.uniform(*self._yaw_range))\n",
    "        pitch = np.deg2rad(self._rng.uniform(*self._tilt_range))\n",
    "        roll = np.deg2rad(self._rng.uniform(*self._tilt_range))\n",
    "        scale = self._rng.uniform(*self._scale)\n",
    "\n",
    "        cy, sy = np.cos(yaw), np.sin(yaw)\n",
    "        cp, sp = np.cos(pitch), np.sin(pitch)\n",
    "        cr, sr = np.cos(roll), np.sin(roll)\n",
    "\n",
    "        rotation_mtx = np.array([[cy*cp,  cy*sp*sr - sy*cr,  cy*sp*cr + sy*sr],\n",
    "                                 [sy*cp,  sy*sp*sr + cy*cr,  sy*sp*cr - cy*sr],\n",
    "                                 [ -sp ,            cp*sr ,            cp*cr ]],\n",
    "                                dtype=coords.dtype)\n",
    "\n",
    "        return (coords @ rotation_mtx.T) * scale\n",
    "        \n",
    "    def _preprocess(self, idx: int):\n",
    "        coords, feat, semantic_labels, instance_labels = self._load_file(self._files[idx % len(self._files)])\n",
    "        if idx >= len(self._files):\n",
    "            coords = self._agument_data(coords)\n",
    "\n",
    "        voxels, indices, inverse_map = sparse_quantize(coords, self._voxel_size, return_index=True, return_inverse=True)\n",
    "        feat = feat[indices]\n",
    "        semantic_labels = semantic_labels[indices]\n",
    "        instance_labels = instance_labels[indices]\n",
    "\n",
    "        voxels = torch.tensor(voxels, dtype=torch.int)\n",
    "        feat = torch.tensor(feat.astype(np.float32), dtype=torch.float)\n",
    "        semantic_labels = torch.tensor(semantic_labels, dtype=torch.long)\n",
    "        instance_labels = torch.tensor(instance_labels, dtype=torch.long)\n",
    "\n",
    "        inputs = SparseTensor(coords=voxels, feats=feat)\n",
    "        semantic_labels = SparseTensor(coords=voxels, feats=semantic_labels)\n",
    "        instance_labels = SparseTensor(coords=voxels, feats=instance_labels)\n",
    "\n",
    "        return {\"inputs\": inputs, \"semantic_labels\": semantic_labels, \"instance_labels\": instance_labels, \"coords\": coords, \"inverse_map\": inverse_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MixedDataset(voxel_size=VOXEL_SIZE, data_augmentation=DATA_AUGMENTATION_COEF)\n",
    "input = dataset.train_dataset[2]\n",
    "inputs = input['inputs']\n",
    "semantic_labels = input['semantic_labels'].F\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "coords = inputs.C\n",
    "# colors = inputs.F\n",
    "\n",
    "colors = dataset.class_colormap[semantic_labels] / 255.0\n",
    "pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeProjectorTrainer:\n",
    "    def __init__(self):\n",
    "        F.set_kmap_mode(\"hashmap\")\n",
    "\n",
    "        self._dataset = MixedDataset(voxel_size=VOXEL_SIZE, data_augmentation=DATA_AUGMENTATION_COEF)\n",
    "\n",
    "        self._model = TreeProjector(self._dataset.feat_channels, self._dataset.num_classes, MAX_INSTANCES, channels = CHANNELS, latent_dim = LATENT_DIM)\n",
    "        # self._model = UNet(self._dataset.feat_channels, self._dataset.num_classes, base_channels=64, depth=3)\n",
    "        self._device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self._model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self._model.parameters() if p.requires_grad)\n",
    "\n",
    "        print(f\"Parámetros totales: {total_params:,}\")\n",
    "        print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
    "\n",
    "        self._train_loader = DataLoader(self._dataset.train_dataset, batch_size=BATCH_SIZE, collate_fn=sparse_collate_fn, shuffle=True)\n",
    "        self._val_loader = DataLoader(self._dataset.val_dataset, batch_size=BATCH_SIZE, collate_fn=sparse_collate_fn, shuffle=True)\n",
    "\n",
    "        self._criterion_semantic = nn.CrossEntropyLoss()\n",
    "        self._criterion_instance = nn.CrossEntropyLoss()\n",
    "\n",
    "        if not TRAINING:\n",
    "            self._load_weights()\n",
    "\n",
    "        self._model.to(self._device)\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self._dataset\n",
    "\n",
    "    def _load_weights(self):\n",
    "        self._model.load_state_dict(torch.load('./weights/tree_unet_weights.pth'))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _apply_hungarian(self, logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        N, K = logits.shape\n",
    "        device = logits.device\n",
    "\n",
    "        log_p = tF.log_softmax(logits, dim=-1)\n",
    "\n",
    "        uniq = torch.unique(labels)\n",
    "        M    = len(uniq)\n",
    "\n",
    "        cost = torch.empty((M, K), device=device)\n",
    "        for m, g in enumerate(uniq):\n",
    "            mask = (labels == g)\n",
    "            cost[m] = -(log_p[mask].mean(0))\n",
    "\n",
    "        row, col = linear_sum_assignment(cost.detach().cpu())\n",
    "\n",
    "        remapped = torch.full_like(labels, fill_value=-1)\n",
    "        for r, c in zip(row, col):\n",
    "            g = uniq[r]\n",
    "            remapped[labels == g] = c\n",
    "\n",
    "        print(f'Originales: {torch.unique(labels)}\\nRemapeadas: {torch.unique(remapped)}')\n",
    "        return remapped\n",
    "\n",
    "    '''\n",
    "    @torch.no_grad()\n",
    "    def _apply_hungarian(conf: torch.Tensor, gt_labels: torch.Tensor, ignore_val: int = -1) -> torch.Tensor:\n",
    "        M, K = conf.shape\n",
    "        row, col = linear_sum_assignment((-conf).cpu().numpy())\n",
    "\n",
    "        remapped = torch.full_like(gt_labels, fill_value=ignore_val)\n",
    "        for r, c in zip(row, col):\n",
    "            remapped[gt_labels == r] = c\n",
    "\n",
    "        return remapped\n",
    "    '''\n",
    "\n",
    "    def _compute_loss(self, semantic_output, semantic_labels, instance_output = 0, instance_labels = 0):\n",
    "        loss_sem = self._criterion_semantic(semantic_output, semantic_labels)\n",
    "        #loss_inst = self._criterion_instance(instance_output, self._apply_hungarian(instance_output, instance_labels))\n",
    "        loss_inst = 0\n",
    "\n",
    "        return SEMANTIC_LOSS_COEF * loss_sem + INSTANCE_LOSS_COEF * loss_inst\n",
    "    \n",
    "    @torch.no_grad()    \n",
    "    def _compute_metrics(self, pred_labels, gt_labels, num_classes, ignore_index = None):\n",
    "        if ignore_index is not None:\n",
    "            mask = gt_labels != ignore_index\n",
    "            pred_labels, gt_labels = pred_labels[mask], gt_labels[mask]\n",
    "\n",
    "        pred_labels = torch.argmax(pred_labels, dim=1)\n",
    "\n",
    "        C = num_classes\n",
    "        conf = torch.zeros((C, C), dtype=torch.long, device=pred_labels.device)\n",
    "        idx = C * gt_labels + pred_labels\n",
    "        conf += torch.bincount(idx, minlength=C**2).reshape(C, C)\n",
    "\n",
    "        TP = conf.diag()\n",
    "        FP = conf.sum(0) - TP\n",
    "        FN = conf.sum(1) - TP\n",
    "\n",
    "        precision = TP.float() / (TP + FP).clamp(min=1)\n",
    "        recall    = TP.float() / (TP + FN).clamp(min=1)\n",
    "        f1        = 2 * precision * recall / (precision + recall).clamp(min=1e-6)\n",
    "\n",
    "        iou = TP.float() / (TP + FP + FN).clamp(min=1)\n",
    "        miou = iou.mean()\n",
    "\n",
    "        macroP, macroR, macroF = precision.mean(), recall.mean(), f1.mean()\n",
    "\n",
    "        microTP = TP.sum()\n",
    "        microP = microTP.float() / (microTP + FP.sum()).clamp(min=1)\n",
    "        microR = microTP.float() / (microTP + FN.sum()).clamp(min=1)\n",
    "        microF = 2 * microP * microR / (microP + microR).clamp(min=1e-6)\n",
    "\n",
    "        return {\n",
    "            \"confusion\":             conf.cpu().numpy(),\n",
    "            \"iou_per_class\":         iou.cpu().numpy(),\n",
    "            \"miou\":                  miou.cpu().numpy(),\n",
    "            \"precision_per_class\":   precision.cpu().numpy(),\n",
    "            \"recall_per_class\":      recall.cpu().numpy(),\n",
    "            \"f1_per_class\":          f1.cpu().numpy(),\n",
    "            \"precision_macro\":       macroP.cpu().numpy(),\n",
    "            \"recall_macro\":          macroR.cpu().numpy(),\n",
    "            \"f1_macro\":              macroF.cpu().numpy(),\n",
    "            \"precision_micro\":       microP.cpu().numpy(),\n",
    "            \"recall_micro\":          microR.cpu().numpy(),\n",
    "            \"f1_micro\":              microF.cpu().numpy(),\n",
    "        }\n",
    "    \n",
    "    '''\n",
    "    def _compute_iou(self, semantic_output, semantic_labels):\n",
    "        if semantic_output.C.shape != semantic_labels.C.shape or not torch.all(semantic_output.C == semantic_labels.C):\n",
    "            raise ValueError(\"Dimensions doesn't match between semantic labels and output.\")\n",
    "\n",
    "        semantic_output = semantic_output.F.argmax(dim=1)\n",
    "        semantic_labels = semantic_labels.F.view(-1).long()\n",
    "        iou_list = torch.full((self._dataset.num_classes,), float('nan'), device=semantic_output.device)\n",
    "\n",
    "        for cls in range(self._dataset.num_classes):\n",
    "            label_mask = semantic_labels == cls\n",
    "            out_mask = semantic_output == cls\n",
    "\n",
    "            union = (out_mask | label_mask).sum()\n",
    "            if union == 0:\n",
    "                continue\n",
    "\n",
    "            inter = (out_mask & label_mask).sum()\n",
    "            iou_list[cls] = inter.float() / union.float()\n",
    "\n",
    "        valid = ~torch.isnan(iou_list)\n",
    "        miou  = iou_list[valid].mean().item() if valid.any() else float(\"nan\")\n",
    "        return iou_list, miou\n",
    "    '''\n",
    "            \n",
    "    def _gen_charts(self, losses, stats, training):\n",
    "        keys = stats[0].keys()\n",
    "        stats = {k: np.array([d[k] for d in stats]) for k in keys}\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses, label=f\"{'Training' if training else 'Inference'} Loss\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Loss evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['miou'], label=f\"{'Training' if training else 'Inference'} mIoU\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"mIoU\")\n",
    "        plt.title(f\"mIoU evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        iou_arr = np.asarray(stats['iou_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(iou_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"IoU\")\n",
    "        plt.title(f\"IoU evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['precision_macro'], label=f\"{'Training' if training else 'Inference'} precision\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Precision evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        prec_arr = np.asarray(stats['precision_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(prec_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"Precision evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['recall_macro'], label=f\"{'Training' if training else 'Inference'} recall\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Recall\")\n",
    "        plt.title(f\"Recall evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        recall_arr = np.asarray(stats['recall_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(recall_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Recall\")\n",
    "        plt.title(f\"Recall evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(stats['f1_macro'], label=f\"{'Training' if training else 'Inference'} F1\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"F1\")\n",
    "        plt.title(f\"F1 evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        f1_arr = np.asarray(stats['f1_per_class'])\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for c in range(self._dataset.num_classes):\n",
    "            plt.plot(f1_arr[:, c], label=self._dataset.class_names[c])\n",
    "        \n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"F1\")\n",
    "        plt.title(f\"F1 evolution during {'Training' if training else 'Inference'}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        column_names = ['IoU', 'Precision', 'Recall', 'F1']\n",
    "        row_names = [self._dataset.class_names[c] for c in range(self._dataset.num_classes)]\n",
    "        row_names.append('Mean')\n",
    "\n",
    "        data = [\n",
    "            [iou_arr[:, c].mean(), prec_arr[:, c].mean(), recall_arr[:, c].mean(), f1_arr[:, c].mean()]\n",
    "        for c in range(self._dataset.num_classes)]\n",
    "\n",
    "        data.append([stats['miou'].mean(), stats['precision_macro'].mean(), stats['recall_macro'].mean(), stats['f1_macro'].mean()])\n",
    "\n",
    "        df = pd.DataFrame(data, columns=column_names, index=row_names)\n",
    "        display(df)\n",
    "    \n",
    "    def train(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), lr=1e-3)\n",
    "        scaler = amp.GradScaler(enabled=True)\n",
    "        losses = []\n",
    "        stats = []\n",
    "\n",
    "        pbar = tqdm(self._train_loader, desc='[Train]')\n",
    "        for feed_dict in pbar:\n",
    "            inputs = feed_dict[\"inputs\"].to(self._device)\n",
    "            semantic_labels = feed_dict[\"semantic_labels\"].to(self._device)\n",
    "            # instance_labels = feed_dict[\"instance_labels\"].to(self._device)\n",
    "\n",
    "            with amp.autocast(enabled=True):\n",
    "                # semantic_output, instance_output = self._model(inputs)\n",
    "                semantic_output = self._model(inputs)\n",
    "                loss = self._compute_loss(semantic_output.F, semantic_labels.F)\n",
    "                # loss = self._compute_loss(semantic_output.F, semantic_labels.F, instance_output.F, instance_labels.F)\n",
    "                stat = self._compute_metrics(semantic_output.F, semantic_labels.F, num_classes=self._dataset.num_classes)\n",
    "\n",
    "\n",
    "            stats.append(stat)\n",
    "            losses.append(loss.item())\n",
    "            # print(f\"[Train step {k + 1}] loss = {loss.item()}; mIoU = {stat['miou']}\")\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'mIoU': f'{stat[\"miou\"]:.4f}'\n",
    "            })\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            del inputs, semantic_output, semantic_labels\n",
    "\n",
    "        torch.save(self._model.state_dict(), './weights/tree_unet_weights.pth')\n",
    "        self._gen_charts(losses, stats, True)\n",
    "\n",
    "    def eval(self):\n",
    "        self._model.eval()\n",
    "        losses = []\n",
    "        stats = []\n",
    "\n",
    "        # enable torchsparse 2.0 inference\n",
    "        # enable fused and locality-aware memory access optimization\n",
    "        torchsparse.backends.benchmark = True  # type: ignore\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self._val_loader, desc='[Inference]')\n",
    "            for feed_dict in pbar:\n",
    "                semantic_labels_cpu = feed_dict[\"semantic_labels\"].F.numpy()\n",
    "                instance_labels_cpu = feed_dict[\"instance_labels\"].F.numpy()\n",
    "                coords = feed_dict[\"coords\"].numpy()\n",
    "                inverse_map = feed_dict[\"inverse_map\"].numpy()\n",
    "\n",
    "                inputs = feed_dict[\"inputs\"].to(self._device)\n",
    "                semantic_labels = feed_dict[\"semantic_labels\"].to(self._device)\n",
    "                # instance_labels = feed_dict[\"instance_labels\"].to(self._device)\n",
    "\n",
    "                with amp.autocast(enabled=True):\n",
    "                    # semantic_output, instance_output = self._model(inputs)\n",
    "                    semantic_output = self._model(inputs)\n",
    "                    loss = self._compute_loss(semantic_output.F, semantic_labels.F)\n",
    "                    # loss = self._compute_loss(semantic_output.F, semantic_labels.F, instance_output.F, instance_labels.F)\n",
    "                    stat = self._compute_metrics(semantic_output.F, semantic_labels.F, num_classes=self._dataset.num_classes)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                stats.append(stat)\n",
    "\n",
    "                voxels = semantic_output.C[:, 1:].cpu().numpy()\n",
    "                semantic_output = torch.argmax(semantic_output.F.cpu(), dim=1).numpy()\n",
    "                # instance_output = torch.argmax(instance_output.F.cpu(), dim=1).numpy()\n",
    "                instance_output = np.zeros(semantic_output.shape)\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'mIoU': f'{stat[\"miou\"]:.4f}'\n",
    "                })\n",
    "\n",
    "                yield voxels, semantic_output, instance_output, semantic_labels_cpu, instance_labels_cpu, coords, inverse_map\n",
    "\n",
    "        self._gen_charts(losses, stats, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros totales: 16,119,472\n",
      "Parámetros entrenables: 16,119,472\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ScanNet' object has no attribute 'train_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tester \u001b[38;5;241m=\u001b[39m \u001b[43mTreeProjectorTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAINING:\n\u001b[1;32m      4\u001b[0m     tester\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mTreeProjectorTrainer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParámetros totales: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParámetros entrenables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, collate_fn\u001b[38;5;241m=\u001b[39msparse_collate_fn, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mval_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, collate_fn\u001b[38;5;241m=\u001b[39msparse_collate_fn, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_criterion_semantic \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ScanNet' object has no attribute 'train_dataset'"
     ]
    }
   ],
   "source": [
    "tester = TreeProjectorTrainer()\n",
    "\n",
    "if TRAINING:\n",
    "    tester.train()\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "for voxels, semantic_output, instance_output, semantic_labels, instance_labels, coords, inverse_map in tester.eval():\n",
    "    continue\n",
    "    coords = coords[0]\n",
    "    inverse_map = inverse_map[0]\n",
    "\n",
    "    colors = tester.dataset.class_colormap[semantic_labels[inverse_map]] / 255.0\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    colors = tester.dataset.class_colormap[semantic_output[inverse_map]] / 255.0\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    unique_ids = np.unique(instance_labels)\n",
    "    rng = np.random.default_rng(0)\n",
    "    palette = rng.random((len(unique_ids), 3))\n",
    "\n",
    "    id2color = {uid: palette[i] for i, uid in enumerate(unique_ids)}\n",
    "    colors = np.array([id2color[i] for i in instance_labels], dtype=np.float64)\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[inverse_map])\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "    unique_ids = np.unique(instance_output)\n",
    "    rng = np.random.default_rng(0)\n",
    "    palette = rng.random((len(unique_ids), 3))\n",
    "\n",
    "    id2color = {uid: palette[i] for i, uid in enumerate(unique_ids)}\n",
    "    colors = np.array([id2color[i] for i in instance_output], dtype=np.float64)\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(coords)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors[inverse_map])\n",
    "    o3d.visualization.draw_geometries([pcd])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
