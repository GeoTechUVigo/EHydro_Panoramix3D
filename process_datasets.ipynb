{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc242f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import laspy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54350d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "datasets_folder = Path(os.environ.get('TREE_PROJECTOR_DIR', Path.home() / 'tree_projector')) / 'datasets'\n",
    "for_instance_folder = (datasets_folder / 'FORinstance_dataset', datasets_folder / 'MixedDataset')\n",
    "for_instance_big_folder = (datasets_folder / 'FORinstance_big_dataset', datasets_folder / 'MixedDataset')\n",
    "nibio_mls_folder = (datasets_folder / 'NIBIO_MLS', datasets_folder / 'MixedDataset')\n",
    "ehydro_folder = (datasets_folder / 'EHydro_raw', datasets_folder / 'EHydro', datasets_folder / 'EHydro_full')\n",
    "\n",
    "for_instance_folder[0].mkdir(parents=True, exist_ok=True)\n",
    "for_instance_folder[1].mkdir(parents=True, exist_ok=True)\n",
    "for_instance_big_folder[0].mkdir(parents=True, exist_ok=True)\n",
    "for_instance_big_folder[1].mkdir(parents=True, exist_ok=True)\n",
    "nibio_mls_folder[0].mkdir(parents=True, exist_ok=True)\n",
    "nibio_mls_folder[1].mkdir(parents=True, exist_ok=True)\n",
    "ehydro_folder[0].mkdir(parents=True, exist_ok=True)\n",
    "ehydro_folder[1].mkdir(parents=True, exist_ok=True)\n",
    "ehydro_folder[2].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mixed_classes = {\n",
    "    'terrain': 0,\n",
    "    # 'low_vegetation': 1,\n",
    "    'stem': 1,\n",
    "    'canopy': 2\n",
    "}\n",
    "\n",
    "ehydro_classes = {\n",
    "    'terrain': 0,\n",
    "    'low_vegetation': 1,\n",
    "    'tree': 2,\n",
    "    'others': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918b4efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_point_clouds(folder):\n",
    "    files = sorted(\n",
    "        [f for f in folder.rglob(\"*\") if f.is_file() and f.suffix.lower() in ('.laz', '.las')],\n",
    "        key=lambda f: f.name\n",
    "    )\n",
    "\n",
    "    for path in tqdm(files, desc=f'Procesando {folder}'):\n",
    "        ext = path.suffix.lower()\n",
    "        file = laspy.read(path)\n",
    "\n",
    "        min_coords = np.array([file.x.min(), file.y.min(), file.z.min()], dtype=np.int64)\n",
    "        mins_world = min_coords * file.header.scales + file.header.offsets\n",
    "        file.header.offsets -= mins_world\n",
    "\n",
    "        intensity = np.array(file.intensity)\n",
    "        min_intensity = np.min(intensity)\n",
    "        max_intensity = np.max(intensity)\n",
    "\n",
    "        file.add_extra_dims([\n",
    "            laspy.ExtraBytesParams(name=\"norm_intensity\", type=np.float32),\n",
    "            laspy.ExtraBytesParams(name=\"semantic_pred\", type=np.int16),\n",
    "            laspy.ExtraBytesParams(name=\"instance_pred\", type=np.int16)\n",
    "        ])\n",
    "\n",
    "        file.norm_intensity = (intensity - min_intensity) / (max_intensity - min_intensity)\n",
    "        yield ext, file\n",
    "\n",
    "def chunkerize(file, chunk_size = 25):\n",
    "    xy = np.stack([file.x, file.y], axis=1)\n",
    "    chunk_idx = np.floor((xy - xy.min(axis=0)) / chunk_size).astype(int)\n",
    "    chunk_keys, inverse = np.unique(chunk_idx, axis=0, return_inverse=True)\n",
    "    \n",
    "    chunk_masks = []\n",
    "    for i in range(len(chunk_keys)):\n",
    "        mask = inverse == i\n",
    "        xy_chunk = xy[mask]\n",
    "\n",
    "        min_chunk = xy_chunk.min(axis=0)\n",
    "        max_chunk = xy_chunk.max(axis=0)\n",
    "        span = max_chunk - min_chunk\n",
    "        \n",
    "        if np.all(span < 0.8 * chunk_size):\n",
    "            continue\n",
    "\n",
    "        chunk_masks.append(mask)\n",
    "\n",
    "    return chunk_masks\n",
    "\n",
    "def chunkerize_clean(file, chunk_size):\n",
    "    xy = np.stack([file.x, file.y], axis=1)\n",
    "    labels = file.instance_pred\n",
    "    unique_labels = np.unique(labels)\n",
    "    unique_labels = unique_labels[unique_labels != 0]\n",
    "\n",
    "    centers = []\n",
    "    for label in unique_labels:\n",
    "        centers.append(xy[labels == label].mean(axis=0))\n",
    "\n",
    "    if not centers:\n",
    "        return [np.ones_like(file.instance_pred, dtype=bool)]\n",
    "    \n",
    "    if len(centers) == 1:\n",
    "        return [(xy[:, 0] >= centers[0][0] - (chunk_size / 2)) & (xy[:, 0] <= centers[0][0] + (chunk_size / 2)) & \\\n",
    "            (xy[:, 1] >= centers[0][1] - (chunk_size / 2)) & (xy[:, 1] <= centers[0][1] + (chunk_size / 2))]\n",
    "\n",
    "    centers = np.array(centers)\n",
    "    while True:\n",
    "        distances = np.abs(centers[None, :] - centers[:, None])\n",
    "        distances = np.min(distances, axis=-1)\n",
    "        mask = np.triu(np.ones_like(distances, dtype=bool), k=1)\n",
    "        idxs = np.argwhere(mask)\n",
    "        vals = distances[mask]\n",
    "        flat_idx = vals.argmin()\n",
    "        val = vals[flat_idx]\n",
    "        if val > (chunk_size / 2) * 0.6:\n",
    "            break\n",
    "\n",
    "        i, j = idxs[flat_idx]\n",
    "        row_proximity = distances[i, :].sum()\n",
    "        col_proximity = distances[:, j].sum()\n",
    "        centers = np.delete(centers, i if row_proximity < col_proximity else j, axis=0)\n",
    "\n",
    "    masks = []\n",
    "    for center in centers:\n",
    "        masks.append((xy[:, 0] >= center[0] - (chunk_size / 2)) & (xy[:, 0] <= center[0] + (chunk_size / 2)) & \\\n",
    "            (xy[:, 1] >= center[1] - (chunk_size / 2)) & (xy[:, 1] <= center[1] + (chunk_size / 2)))\n",
    "        \n",
    "    return masks\n",
    "\n",
    "def chunkerize_four(file):\n",
    "    xy = np.stack([file.x, file.y], axis=1)\n",
    "    center = xy.mean(axis=0)\n",
    "\n",
    "    return [\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] > center[1]),\n",
    "        (xy[:, 0] < center[0]) & (xy[:, 1] < center[1]),\n",
    "        (xy[:, 0] > center[0]) & (xy[:, 1] < center[1])\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b243e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando /home/samuel/tree_projector/datasets/FORinstance_dataset: 100%|██████████| 28/28 [01:09<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "for_instance_classes = {\n",
    "    'unclassified': 0,\n",
    "    'low_vegetation': 1,\n",
    "    'terrain': 2,\n",
    "    'out_points': 3,\n",
    "    'stem': 4,\n",
    "    'live_branches': 5,\n",
    "    'woody_branches': 6\n",
    "}\n",
    "\n",
    "for i, (ext, file) in enumerate(load_point_clouds(for_instance_folder[0])):\n",
    "    mask = (file.classification != for_instance_classes['out_points']) & (file.classification != for_instance_classes['unclassified'])  # Eliminamos puntos no clasificados o inválidos\n",
    "    file.points = file.points[mask]\n",
    "\n",
    "    semantic_labels = np.array(file.classification)\n",
    "    remap = np.copy(semantic_labels)\n",
    "\n",
    "    remap = np.where(semantic_labels == for_instance_classes['low_vegetation'], mixed_classes['terrain'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['terrain'], mixed_classes['terrain'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['stem'], mixed_classes['stem'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['live_branches'], mixed_classes['canopy'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['woody_branches'], mixed_classes['canopy'], remap)\n",
    "\n",
    "    file.semantic_pred = remap\n",
    "    file.instance_pred = file.treeID\n",
    "    chunk_masks = chunkerize_four(file)\n",
    "    for j, mask in enumerate(chunk_masks):\n",
    "        pts_chunk = file.points[mask]\n",
    "        instance_labels_chunk = file.treeID[mask]\n",
    "        unique_vals, inv = np.unique(instance_labels_chunk, return_inverse=True)\n",
    "\n",
    "        out = laspy.create(point_format=file.point_format, file_version=file.header.version)\n",
    "        out.header.scales = file.header.scales\n",
    "        out.header.offsets = file.header.offsets\n",
    "\n",
    "        out.points = pts_chunk\n",
    "        out.instance_pred = inv\n",
    "        out.write(for_instance_folder[1] / f'plot_FORinstance_{i}_{j}.las')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4d21f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando /home/samuel/tree_projector/datasets/FORinstance_big_dataset: 100%|██████████| 4/4 [00:10<00:00,  2.68s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, (ext, file) in enumerate(load_point_clouds(for_instance_big_folder[0])):\n",
    "    mask = (file.classification != for_instance_classes['out_points']) & (file.classification != for_instance_classes['unclassified'])  # Eliminamos puntos no clasificados o inválidos\n",
    "    file.points = file.points[mask]\n",
    "\n",
    "    semantic_labels = np.array(file.classification)\n",
    "    remap = np.copy(semantic_labels)\n",
    "\n",
    "    remap = np.where(semantic_labels == for_instance_classes['low_vegetation'], mixed_classes['terrain'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['terrain'], mixed_classes['terrain'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['stem'], mixed_classes['stem'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['live_branches'], mixed_classes['canopy'], remap)\n",
    "    remap = np.where(semantic_labels == for_instance_classes['woody_branches'], mixed_classes['canopy'], remap)\n",
    "\n",
    "    file.semantic_pred = remap\n",
    "    file.instance_pred = file.treeID\n",
    "    chunk_masks = chunkerize_clean(file, chunk_size=12.5)\n",
    "    for j, mask in enumerate(chunk_masks):\n",
    "        pts_chunk = file.points[mask]\n",
    "        instance_labels_chunk = file.treeID[mask]\n",
    "        unique_vals, inv = np.unique(instance_labels_chunk, return_inverse=True)\n",
    "\n",
    "        out = laspy.create(point_format=file.point_format, file_version=file.header.version)\n",
    "        out.header.scales = file.header.scales\n",
    "        out.header.offsets = file.header.offsets\n",
    "\n",
    "        out.points = pts_chunk\n",
    "        out.instance_pred = inv\n",
    "        out.write(for_instance_big_folder[1] / f'plot_FORinstance_big_{i}_{j}.las')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "075f2421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando /home/samuel/tree_projector/datasets/NIBIO_MLS: 100%|██████████| 64/64 [00:48<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "nibio_mls_classes = {\n",
    "    'ground': 1,\n",
    "    'vegetation': 2,\n",
    "    'lying_deadwood': 3,\n",
    "    'stems': 4\n",
    "}\n",
    "\n",
    "for i, (ext, file) in enumerate(load_point_clouds(nibio_mls_folder[0])):\n",
    "    semantic_labels = np.array(file.label)\n",
    "    remap = np.copy(semantic_labels)\n",
    "\n",
    "    remap = np.where(semantic_labels == nibio_mls_classes['ground'], mixed_classes['terrain'], remap)\n",
    "    remap = np.where((semantic_labels == nibio_mls_classes['vegetation']) & (file.treeID == 0), mixed_classes['terrain'], remap)\n",
    "    remap = np.where((semantic_labels == nibio_mls_classes['vegetation']) & (file.treeID != 0), mixed_classes['canopy'], remap)\n",
    "    remap = np.where(semantic_labels == 3, mixed_classes['terrain'], remap)\n",
    "    remap = np.where(semantic_labels == 4, mixed_classes['stem'], remap)\n",
    "\n",
    "    file.semantic_pred = remap\n",
    "    file.instance_pred = file.treeID\n",
    "    file.write(nibio_mls_folder[1] / f'plot_NIBIO_MLS_{i}{ext}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb3c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [04:15, 31.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks a priori: 1120\n",
      "Chunks completos, con al menos un 80% del tamaño requerido: 1021 (91.16%)\n",
      "Chunks válidos, con al menos tres instancias y tres clases presentes: 615 (54.91%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "a_priori_chunks = 0\n",
    "complete_chunks = 0\n",
    "valid_chunks = 0\n",
    "\n",
    "for i, (ext, file) in enumerate(load_point_clouds(ehydro_folder[0])):\n",
    "    semantic_labels = np.array(file.classification)\n",
    "    remap = np.full_like(semantic_labels, ehydro_classes['others'])\n",
    "    instance_labels = np.array(file.PredInstance)\n",
    "    \n",
    "    ids = np.unique(instance_labels)\n",
    "    ids = ids[ids != 0]\n",
    "\n",
    "    remap = np.where(instance_labels == 0, ehydro_classes['terrain'], remap)\n",
    "    for id in ids:\n",
    "        mask = instance_labels == id\n",
    "        z = np.asarray(file.z[mask])\n",
    "        if z.mean() - z.min() > 6.0:\n",
    "            remap[mask] = ehydro_classes['tree']\n",
    "        else:\n",
    "            remap[mask] = ehydro_classes['low_vegetation']\n",
    "\n",
    "    remap = np.where(semantic_labels == 6, ehydro_classes['others'], remap)\n",
    "    file.semantic_pred = remap\n",
    "    file.instance_pred = instance_labels\n",
    "\n",
    "    chunk_masks = chunkerize(file)\n",
    "    for mask in chunk_masks:\n",
    "        pts_chunk = file.points[mask]\n",
    "        instance_labels_chunk = instance_labels[mask]\n",
    "        semantic_pred_chunk = remap[mask]\n",
    "\n",
    "        uniq = np.unique(instance_labels_chunk)\n",
    "        if len(uniq) < 5 or len(np.unique(semantic_pred_chunk)) < 3:\n",
    "            continue\n",
    "\n",
    "        instance_labels_chunk = uniq.searchsorted(instance_labels_chunk)\n",
    "\n",
    "        out = laspy.create(point_format=file.point_format, file_version=file.header.version)\n",
    "        out.header.scales = file.header.scales\n",
    "        out.header.offsets = file.header.offsets\n",
    "\n",
    "        out.points = pts_chunk\n",
    "        out.instance_pred = instance_labels_chunk\n",
    "        out.write(ehydro_folder[1] / f'plot_ehydro_{i}_{i}.las')\n",
    "    \n",
    "    file.write(ehydro_folder[2] / f'plot_ehydro_{i}.las')\n",
    "\n",
    "print(f'Chunks a priori: {a_priori_chunks}')\n",
    "print(f'Chunks completos, con al menos un 80% del tamaño requerido: {complete_chunks} ({((complete_chunks / a_priori_chunks) * 100):.2f}%)')\n",
    "print(f'Chunks válidos, con al menos tres instancias y tres clases presentes: {valid_chunks} ({((valid_chunks / a_priori_chunks) * 100):.2f}%)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
